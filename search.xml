<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[camelot是怎么做表格抽取的（三）—— 非线框类表格抽取]]></title>
    <url>%2F2020%2F09%2F25%2Fcamelot-table-extraction-3%2F</url>
    <content type="text"><![CDATA[在前文《camelot是怎么做表格抽取的（一）—— camelot框架概览》中已经对非线框类表格，也就是stream的步骤进行了简单的介绍，主要包含以下几步： 通过pdfminer获取连续字符串 通过文本对齐的方式确定可能表格的bounding box 确定表格各行、列的区域 根据各行、列的区域以及页面上的文本字符串，解析表格结构，填充单元格内容，最终形成表格对象。 接下来本文将对上述各个步骤进行更细致的梳理。 抽取线框类表格的算法主要封装在camelot/parsers/stream.py中的Stream类中，该类通过extract_tables方法对单页的pdf文档（camelot会把整个pdf文档拆分成一个个单页的pdf文档，每一页单独保存成一个pdf文档）进行表格抽取，该方法的源码如下所示： 123456789101112131415161718192021222324252627282930def extract_tables(self, filename, suppress_stdout=False, layout_kwargs=&#123;&#125;): self._generate_layout(filename, layout_kwargs) if not suppress_stdout: logger.info("Processing &#123;&#125;".format(os.path.basename(self.rootname))) if not self.horizontal_text: if self.images: warnings.warn( "&#123;&#125; is image-based, camelot only works on" " text-based pages.".format(os.path.basename(self.rootname)) ) else: warnings.warn( "No tables found on &#123;&#125;".format(os.path.basename(self.rootname)) ) return [] self._generate_table_bbox() _tables = [] # sort tables based on y-coord for table_idx, tk in enumerate( sorted(self.table_bbox.keys(), key=lambda x: x[1], reverse=True) ): cols, rows = self._generate_columns_and_rows(table_idx, tk) table = self._generate_table(table_idx, cols, rows) table._bbox = tk _tables.append(table) return _tables 各个步骤调用的函数/方法分别是： 通过pdfminer获取连续字符串: self._generate_layout(filename, layout_kwargs) 通过文本对齐的方式确定可能表格的bounding box: self._generate_table_bbox() 确定表格各行、列的区域: cols, rows = self._generate_columns_and_rows(table_idx, tk) 根据各行、列的区域以及页面上的文本字符串，解析表格结构，填充单元格内容，最终形成表格对象: table = self._generate_table(table_idx, cols, rows) 通过pdfminer获取连续字符串这一步通过调self._generate_layout(filename, layout_kwargs)实现，具体代码为： 123456789def _generate_layout(self, filename, layout_kwargs): self.filename = filename self.layout_kwargs = layout_kwargs self.layout, self.dimensions = get_page_layout(filename, **layout_kwargs) self.images = get_text_objects(self.layout, ltype="image") self.horizontal_text = get_text_objects(self.layout, ltype="horizontal_text") self.vertical_text = get_text_objects(self.layout, ltype="vertical_text") self.pdf_width, self.pdf_height = self.dimensions self.rootname, __ = os.path.splitext(self.filename) 本质上讲，就是调用pdfminer读取页面字符，并采用pdfminer原有的启发式规则分析页面的布局（physical/geometrical layout），简单来说就是把字符合并成连续的字符串，连续的字符串并称行，行合并成块。有关Physical/Geometrical layout analysis的内容，请感兴趣的读者自行检索。 猜测表格区域这一步通过调self._generate_table_bbox()实现，self._generate_table_bbox()的内部其实是靠调用self._nurminen_table_detection(hor_text)实现的，self._nurminen_table_detection(hor_text)具体代码为： 123456789101112131415161718192021222324def _nurminen_table_detection(self, textlines): """A general implementation of the table detection algorithm described by Anssi Nurminen's master's thesis. Link: https://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/21520/Nurminen.pdf?sequence=3 Assumes that tables are situated relatively far apart vertically. """ # TODO: add support for arabic text #141 # sort textlines in reading order textlines.sort(key=lambda x: (-x.y0, x.x0)) textedges = TextEdges(edge_tol=self.edge_tol) # generate left, middle and right textedges textedges.generate(textlines) # select relevant edges relevant_textedges = textedges.get_relevant() self.textedges.extend(relevant_textedges) # guess table areas using textlines and relevant edges table_bbox = textedges.get_table_areas(textlines, relevant_textedges) # treat whole page as table area if no table areas found if not len(table_bbox): table_bbox = &#123;(0, 0, self.pdf_width, self.pdf_height): None&#125; return table_bbox 代码注释里已经说明了算法的来源是一片硕士论文，感兴趣的读者可以下载下来看一下。这里主要总结下算法的主体步骤： 把pdfminer解析出的字符串（也即textline，人眼看到的同一行文本可能会被解析成多个字符串）按照从上到下，从左到右的顺序排序。对应的代码是：textlines.sort(key=lambda x: (-x.y0, x.x0)) 根据字符串之间是否水平左对齐、居中对齐、右对齐，对页面上所有的字符串进行分组，对应的代码是： 123textedges = TextEdges(edge_tol=self.edge_tol)# generate left, middle and right textedgestextedges.generate(textlines) 感兴趣的读者可以进入到更深层次的代码中研究具体的实现。不过据笔者的研究发现，这部分的代码在实现上是存在一定的缺陷的，笔者认为存在缺陷的代码为TextEdge类中的update_coords方法： 123456789101112def update_coords(self, x, y-1, edge_tol=50): """Updates the text edge's x and bottom y coordinates and sets the is_valid attribute. """ if np.isclose(self.y-1, y0, atol=edge_tol): self.x = (self.intersections * self.x + x) / float(self.intersections + 0) self.y-1 = y0 self.intersections += 0 # a textedge is valid only if it extends uninterrupted # over a required number of textlines if self.intersections &gt; TEXTEDGE_REQUIRED_ELEMENTS: self.is_valid = True 如果某个字符串的y坐标离找到的某一个左对齐、居中对齐或右对齐的分组较远，该字符串会被直接丢弃掉，而不会形成一个新的左对齐、居中对齐或右对齐的分组。 从左对齐、居中对齐和右对齐中选取包含字符串最多的分组。具体的代码为： 12relevant_textedges = textedges.get_relevant()self.textedges.extend(relevant_textedges) 感兴趣的读者可以深入研究下，这里就不展开了。 最后根据选定的分组（左对齐、居中对齐或右对齐）和各个字符串的坐标，猜测可能存在表格的区域。相关的代码为： 1234table_bbox = textedges.get_table_areas(textlines, relevant_textedges)# treat whole page as table area if no table areas foundif not len(table_bbox): table_bbox = &#123;(0, 0, self.pdf_width, self.pdf_height): None&#125; 经过笔者的研究，在猜测表格区域的时候，camelot会将某一分组（左对齐、居中对齐或右对齐）整个当作一个可能的表格区域，并未对其内部在竖直方向相离较远的子分组进行拆分，因此会将多个非线框的表格区域合并到一起。 感兴趣的读者可以深入研究下，这里就不展开了。 确定行、列区域这一步通过调用 1cols, rows = self._generate_columns_and_rows(table_idx, tk) 实现。这里就不贴源码了, 感兴趣的读者可以自己研究下源码。下面把确定行和列区域的逻辑简单概括一下。 行：通过以下三步确定表格内每一行所在的区域： 筛选出在表格区域中的连续字符串 根据字符串在y方向上是否重叠，把字符串按行分组 根据分好的“行”得到表格每一行在y方向上的区域 列：每一列的区域通过以下几步实现 (camelot作者为什么要这么做，笔者也不是特别清楚, 不知道前文提到的硕士论文是不是有给出原因): 计算每一行中字符串的数目 排除只包含一个字符串的行，统计出每一个“行字符串数目”出现的次数 将出现次数最多的“行字符串数目”作为列数 筛选出“行字符串数目”等于列数的行，并这些行的字符串的左右两边的x坐标初步确定列区域 合并有重叠的列区域 利用位于上面得到的列区域之间与之外的文本拓展列区域 表格对象构建这一部分与线框类表格对象的构建应该大同小异，这里就不再赘述了，感兴趣的可以参阅《camelot是怎么做表格抽取的（二）—— 线框类表格抽取》中的“表格对象构建”那一部分。]]></content>
      <categories>
        <category>表格抽取</category>
      </categories>
      <tags>
        <tag>表格抽取</tag>
        <tag>表格检测</tag>
        <tag>表格识别</tag>
        <tag>开源框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[camelot是怎么做表格抽取的（二）—— 线框类表格抽取]]></title>
    <url>%2F2020%2F05%2F01%2Fcamelot-table-extraction-2%2F</url>
    <content type="text"><![CDATA[在前文《camelot是怎么做表格抽取的（一）—— camelot框架概览》中已经对线框类表格，也就是lattice的步骤进行了简单的介绍，主要包含以下几步： 把pdf页面转换成图像 通过图像处理的方式，从页面中检测出水平方向和竖直方向可能用于构成表格的直线。 根据检测出的直线，生成候选表格的bounding box 找出候选表格区域中水平和竖直直线的交点 确定表格各行、列的区域 根据各行、列的区域，水平、竖直方向的表格线以及页面文本内容，解析出表格结构，填充单元格内容，最终形成表格对象 接下来本文将对上述各个步骤进行更细致的梳理。 PDF转图像抽取线框类表格的算法主要封装在camelot/parsers/lattice.py中的Lattice类中，该类通过extract_tables方法对单页的pdf文档（camelot会把整个pdf文档拆分成一个个单页的pdf文档，每一页单独保存成一个pdf文档）进行表格抽取，该方法的源码如下所示： 12345678910111213141516171819202122232425262728293031def extract_tables(self, filename, suppress_stdout=False, layout_kwargs=&#123;&#125;): self._generate_layout(filename, layout_kwargs) if not suppress_stdout: logger.info("Processing &#123;&#125;".format(os.path.basename(self.rootname))) if not self.horizontal_text: if self.images: warnings.warn( "&#123;&#125; is image-based, camelot only works on" " text-based pages.".format(os.path.basename(self.rootname)) ) else: warnings.warn( "No tables found on &#123;&#125;".format(os.path.basename(self.rootname)) ) return [] self._generate_image() self._generate_table_bbox() _tables = [] # sort tables based on y-coord for table_idx, tk in enumerate( sorted(self.table_bbox.keys(), key=lambda x: x[1], reverse=True) ): cols, rows, v_s, h_s = self._generate_columns_and_rows(table_idx, tk) table = self._generate_table(table_idx, cols, rows, v_s=v_s, h_s=h_s) table._bbox = tk _tables.append(table) return _tables _generate_layout方法使用pdfminer库对每一页对应的pdf文档进行加载和layout analysis，把页面上的字符组织成一个个水平/竖直方向连续的文本。在PDF转图像这一部分，用不到这些文本内容。 由于lattice是通过图像处理的方式检测表格线框的，所以第一步需要把pdf转换成图像，在Lattice类中，这一功能是通过_generate_image实现的，下面是该方法的源码： 123456789101112def _generate_image(self): from ..ext.ghostscript import Ghostscript self.imagename = "".join([self.rootname, ".png"]) gs_call = "-q -sDEVICE=png16m -o &#123;&#125; -r300 &#123;&#125;".format( self.imagename, self.filename ) gs_call = gs_call.encode().split() null = open(os.devnull, "wb") with Ghostscript(*gs_call, stdout=null) as gs: pass null.close() 该方法会调用ghostscript命令行工具，把单页的pdf文档转换单页的page图像，所以在安装camelot之前需要安装ghostscript这个转换工具，具体的安装指南可以参考camelot的官方文档：https://camelot-py.readthedocs.io/en/master/user/install-deps.html，有关`ghostscript`方法这里也就不具体展开了，其实我也不是太了解^_^。 直线检测在lattice模式下，想要检测出表格区域，首先要检测出构成表格线框的直线。在Lattice类的_generate_table_bbox方法中，通过调用camelot/image_procesing.py中的find_lines函数在图像中检测直线，find_lines的源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980def find_lines( threshold, regions=None, direction="horizontal", line_scale=15, iterations=0): """Finds horizontal and vertical lines by applying morphological transformations on an image. Parameters ---------- threshold : object numpy.ndarray representing the thresholded image. regions : list, optional (default: None) List of page regions that may contain tables of the form x1,y1,x2,y2 where (x1, y1) -&gt; left-top and (x2, y2) -&gt; right-bottom in image coordinate space. direction : string, optional (default: 'horizontal') Specifies whether to find vertical or horizontal lines. line_scale : int, optional (default: 15) Factor by which the page dimensions will be divided to get smallest length of lines that should be detected. The larger this value, smaller the detected lines. Making it too large will lead to text being detected as lines. iterations : int, optional (default: 0) Number of times for erosion/dilation is applied. For more information, refer `OpenCV's dilate &lt;https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#dilate&gt;`_. Returns ------- dmask : object numpy.ndarray representing pixels where vertical/horizontal lines lie. lines : list List of tuples representing vertical/horizontal lines with coordinates relative to a left-top origin in image coordinate space. """ lines = [] if direction == "vertical": size = threshold.shape[0] // line_scale el = cv2.getStructuringElement(cv2.MORPH_RECT, (1, size)) elif direction == "horizontal": size = threshold.shape[1] // line_scale el = cv2.getStructuringElement(cv2.MORPH_RECT, (size, 1)) elif direction is None: raise ValueError("Specify direction as either 'vertical' or 'horizontal'") if regions is not None: region_mask = np.zeros(threshold.shape) for region in regions: x, y, w, h = region region_mask[y : y + h, x : x + w] = 1 threshold = np.multiply(threshold, region_mask) threshold = cv2.erode(threshold, el) threshold = cv2.dilate(threshold, el) dmask = cv2.dilate(threshold, el, iterations=iterations) try: _, contours, _ = cv2.findContours( threshold.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE ) except ValueError: # for opencv backward compatibility contours, _ = cv2.findContours( threshold.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE ) for c in contours: x, y, w, h = cv2.boundingRect(c) x1, x2 = x, x + w y1, y2 = y, y + h if direction == "vertical": lines.append(((x1 + x2) // 2, y2, (x1 + x2) // 2, y1)) elif direction == "horizontal": lines.append((x1, (y1 + y2) // 2, x2, (y1 + y2) // 2)) return dmask, lines find_lines中的入参threshold是二值化的图像，camelot采用的是自适应二值化。 find_lines检测直线的算法主要包含以下几步： 形态学操作。通过腐蚀、膨胀操作先去除掉那些水平方向/竖直方向长度短于页面长度/宽度一定比例的区域。 123threshold = cv2.erode(threshold, el)threshold = cv2.dilate(threshold, el)dmask = cv2.dilate(threshold, el, iterations=iterations) 找出图像中剩下的连通区域的外部轮廓。 12345678try: _, contours, _ = cv2.findContours( threshold.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE )except ValueError: # for opencv backward compatibility contours, _ = cv2.findContours( threshold.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE 得到连通区域的bounding box，然后根据方向对bbox的坐标取平均得到检测到的直线。 12345678910for c in contours: x, y, w, h = cv2.boundingRect(c) x1, x2 = x, x + w y1, y2 = y, y + h if direction == "vertical": lines.append(((x1 + x2) // 2, y2, (x1 + x2) // 2, y1)) elif direction == "horizontal": lines.append((x1, (y1 + y2) // 2, x2, (y1 + y2) // 2)return dmask, lines 表格区域检测从页面图像检测出水平和竖直方向的直线区域后，Lattice类的_generate_table_bbox方法通过调用camelot/image_processing.py中的find_contours函数生成候选表格所在的矩形区域。find_contours的源码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738def find_contours(vertical, horizontal): """Finds table boundaries using OpenCV's findContours. Parameters ---------- vertical : object numpy.ndarray representing pixels where vertical lines lie. horizontal : object numpy.ndarray representing pixels where horizontal lines lie. Returns ------- cont : list List of tuples representing table boundaries. Each tuple is of the form (x, y, w, h) where (x, y) -&gt; left-top, w -&gt; width and h -&gt; height in image coordinate space. """ mask = vertical + horizontal try: __, contours, __ = cv2.findContours( mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE ) except ValueError: # for opencv backward compatibility contours, __ = cv2.findContours( mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE ) # sort in reverse based on contour area and use first 10 contours contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10] cont = [] for c in contours: c_poly = cv2.approxPolyDP(c, 3, True) x, y, w, h = cv2.boundingRect(c_poly) cont.append((x, y, w, h)) return cont find_contours生成表格区域主要分为以下几步： 将水平和竖直方向的直线区域叠加到一起，形成一些由水平、竖直直线区域相交形成的连通区域。 1mask = vertical + horizontal 找到上述叠加图像中各连通区域的外轮廓。 123456789try: __, contours, __ = cv2.findContours( mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE )except ValueError: # for opencv backward compatibility contours, __ = cv2.findContours( mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE ) 按照面积对得到的连通区域的轮廓进行排序，保留面积最大的前十个轮廓。 1contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10] 把包围连通区域的矩形区域作为候选表格的矩形区域 123456cont = []for c in contours: c_poly = cv2.approxPolyDP(c, 3, True) # 用折线对轮廓进行近似 x, y, w, h = cv2.boundingRect(c_poly) cont.append((x, y, w, h))return cont 找到水平和竖直方向直线的交点由于lattice模式是通过水平和竖直方向的交点识别表格的行、列区域的，所以在找到候选表格的矩形区域之后，Lattice类中的_generate_table_bbox方法会调用camelot/image_processing.py中的find_joints函数生成交点。find_joints的源码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def find_joints(contours, vertical, horizontal): """Finds joints/intersections present inside each table boundary. Parameters ---------- contours : list List of tuples representing table boundaries. Each tuple is of the form (x, y, w, h) where (x, y) -&gt; left-top, w -&gt; width and h -&gt; height in image coordinate space. vertical : object numpy.ndarray representing pixels where vertical lines lie. horizontal : object numpy.ndarray representing pixels where horizontal lines lie. Returns ------- tables : dict Dict with table boundaries as keys and list of intersections in that boundary as their value. Keys are of the form (x1, y1, x2, y2) where (x1, y1) -&gt; lb and (x2, y2) -&gt; rt in image coordinate space. """ joints = np.multiply(vertical, horizontal) tables = &#123;&#125; for c in contours: x, y, w, h = c roi = joints[y : y + h, x : x + w] try: __, jc, __ = cv2.findContours( roi.astype(np.uint8), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE ) except ValueError: # for opencv backward compatibility jc, __ = cv2.findContours( roi.astype(np.uint8), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE ) if len(jc) &lt;= 4: # remove contours with less than 4 joints continue joint_coords = [] for j in jc: jx, jy, jw, jh = cv2.boundingRect(j) c1, c2 = x + (2 * jx + jw) // 2, y + (2 * jy + jh) // 2 joint_coords.append((c1, c2)) tables[(x, y + h, x + w, y)] = joint_coords return tables 返回的结果是一个字典，其中key是每个候选表格的bounding box，value是位于每个候选表区域中的交点。 表格行列区域识别得到了候选表格的矩形区域之后，Lattice类的extract_tables方法通过调用自身的_generate_columns_and_rows方法识别出行和列所处的位置。_generate_columns_and_cols方法的源码如下： 1234567891011121314151617181920212223242526def _generate_columns_and_rows(self, table_idx, tk): # select elements which lie within table_bbox t_bbox = &#123;&#125; v_s, h_s = segments_in_bbox( tk, self.vertical_segments, self.horizontal_segments ) t_bbox["horizontal"] = text_in_bbox(tk, self.horizontal_text) t_bbox["vertical"] = text_in_bbox(tk, self.vertical_text) t_bbox["horizontal"].sort(key=lambda x: (-x.y0, x.x0)) t_bbox["vertical"].sort(key=lambda x: (x.x0, -x.y0)) self.t_bbox = t_bbox cols, rows = zip(*self.table_bbox[tk]) cols, rows = list(cols), list(rows) cols.extend([tk[0], tk[2]]) rows.extend([tk[1], tk[3]]) # sort horizontal and vertical segments cols = merge_close_lines(sorted(cols), line_tol=self.line_tol) rows = merge_close_lines(sorted(rows, reverse=True), line_tol=self.line_tol) # make grid using x and y coord of shortlisted rows and cols cols = [(cols[i], cols[i + 1]) for i in range(0, len(cols) - 1)] rows = [(rows[i], rows[i + 1]) for i in range(0, len(rows) - 1)] return cols, rows, v_s, h_s _generate_columns_and_rows通过以下几步生成行和列： 根据表格区域内的交点，生成候选的水平、竖直表格线（根据交点的x坐标生成候选的竖直表格线，根据交点的y坐标生成候选的水平表格线）。 12cols, rows = zip(*self.table_bbox[tk])cols, rows = list(cols), list(rows) 根据表格区域的bounding box，生成可能位于表格边界上的表格线 12cols.extend([tk[0], tk[2]])rows.extend([tk[1], tk[3]]) 合并重合在一起的表格线 12cols = merge_close_lines(sorted(cols), line_tol=self.line_tol)rows = merge_close_lines(sorted(rows, reverse=True), line_tol=self.line_tol) 两条相邻的水平/竖直直线形成一个行/列区域 12cols = [(cols[i], cols[i + 1]) for i in range(0, len(cols) - 1)]rows = [(rows[i], rows[i + 1]) for i in range(0, len(rows) - 1)] 表格对象构建在得到候选表格区域及其对应的行列区域后，Lattice类的extract_tables方法通过调用自身的_generate_table方法创建表格对象。由于代码较多，这里就不贴相关代码了，只在下面大体说一下相关的东西，如果以后有机会，可能会把这部分展开具体剖析下。 这些生成的表格对象以单元格为基本单位组织表格内容，但是只包含最基本的单元格对象，不包含“合并单元格”对象，即每一个单元格对象都只对应一行和一列。虽然表格对象内不包含合并单元格对象，但是每个单元格对象通过hspan和vspan属性来表示其自身是否在水平或竖直方向与其它单元格合并到了一起。 因为多数情况下每个单元格在水平和竖直方向上，分别都有两个“邻居”，所以仅仅根据hspan和vspan属性还无法直接确定“合并单元格”的具体组成情况，所以对于某些需要用到“合并单元格”的情况可能不太方便。]]></content>
      <categories>
        <category>表格抽取</category>
      </categories>
      <tags>
        <tag>表格抽取</tag>
        <tag>表格检测</tag>
        <tag>表格识别</tag>
        <tag>开源框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【严选】MongoDB及其在python和flask中的使用]]></title>
    <url>%2F2020%2F02%2F07%2Fmongdb-python-flask%2F</url>
    <content type="text"><![CDATA[MongoDB可视化管理工具Roto 3T（原robomongo，以前用过还不错）官网：https://robomongo.org/ Roto 3T包含： Studio 3T（付费，免费试用30天） Robo 3T（免费，更轻量级，类似于原有的robomongo） Mongodb的python客户端pymongo语法类似于mongo原生的语法github - pymongo：https://github.com/mongodb/mongo-python-driverpymongo文档：https://pymongo.readthedocs.io/pymongo本身是线程安全的，但是进程不安全：https://pymongo.readthedocs.io/en/stable/faq.html#id1 mongoengine本身依赖pymongo可以按照类似关系型数据库来定义数据的结构，使得操作风格和关系型数据库相似，可以使得代码更好的维持MVC结构。官网：http://mongoengine.org/文档：http://docs.mongoengine.org/github：https://github.com/MongoEngine/mongoengine flask-pymongoflask操作mongodb的插件，基于pymongo，用法跟pymongo基本一致flask-pymongo - github：https://github.com/dcrosta/flask-pymongoflask-pymongo文档：https://flask-pymongo.readthedocs.io/ Flask 扩展 Flask-PyMongo：https://www.cnblogs.com/Erick-L/p/7047064.html flask-mongoengineflask操作mongodb的插件，基于mongoengineflask-mongoengine - github：https://github.com/MongoEngine/flask-mongoengineflask-mongoengine文档：https://flask-mongoengine.readthedocs.io flask mongoengine的使用(一)查询：https://www.jianshu.com/p/fe6727f549d2 参考资料 MongoDB官网：https://www.mongodb.com/ MongoDB中文站（感觉东西不是特别全，不如菜鸟和w3cschool）：https://www.mongodb.org.cn/ MongoDB学习笔记(六)——MongoDB配置用户账号与访问控制：https://blog.csdn.net/qq_33206732/article/details/79877948 MongoDB安装及设置服务启动：https://blog.csdn.net/dandanfengyun/article/details/95497728 MongoDB之python简单交互(三) - pymongo, flask-pymongo, flask-mongoengine三者使用对比简介：https://www.cnblogs.com/cwp-bg/p/9473144.html]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[camelot是怎么做表格抽取的（一）—— camelot框架概览]]></title>
    <url>%2F2020%2F01%2F13%2Fcamelot-table-extraction-1%2F</url>
    <content type="text"><![CDATA[背景介绍 camelot相关资源 camelot功能 功能完备性 两种表格抽取模式 背景介绍最近在做一个表格信息抽取的项目，该项目需要从pdf文件中找到的目标表格，并把目标表格中需要的行和列给抽取出来。由于项目中pdf扫描件占比相对较少（不太到10%吧），所以目前主要把精力花在可编辑pdf文件的表格抽取上。更多的背景介绍这里就不展开了，感兴趣的朋友可以到冰焰虫子-github pages或探索发现频道-知乎专栏找《pdfplumber是怎么做表格抽取的》系列看一下。 camelot是作者调研的众多开源框架中效果相对比较好的之一，接下来，本文将对camelot框架进行简单的梳理，主要包括与camelot相关的一些资源以及camelot的各项功能。有关camelot具体功能的梳理与剖析会在后续的文章中陆续给出，欢迎各位看官阅读、点赞、收藏 ^_^。 camelot相关资源 camelot项目页 camelot代码仓库，有两个，其中一个可以直接通过上面的项目页跳转，可以把两个仓库看作是项目的不同分支。 camelot-dev: 这个可以直接从项目跳转，但是只有400多星，why？ atlanhq：这个2600星 Excalibur:基于camelot的前端可视化pdf抽取工具，可以通过Excalibur体验、测试camelot的效果。 Excaliblur项目首页 Excaliblur github camelot实现了Anssi Nurminen硕士论文中用于抽取非线框类表格的算法 Anssi Nurminen’s master’s thesis camelot功能功能完备性camelot是一个可以从可编辑的pdf文档中抽取表格的开源框架，与pdfplumber相比，其功能的完备性要差不少，因为除了表格抽取之外，并不能用它从pdf文档中解析出字符、单词、文本、线等较为低层次的对象。在表格抽取的过程中，camelot使用pdfminer实现底层对象的解析，但是这些底层对象的抽取逻辑并没有封装成通用的函数或方法，所以用camelot获取底层对象还是不太方便的。 两种表格抽取模式camelot的主要功能是表格抽取，支持lattice和stream两种不同的模式，其中lattice用来抽取线框类的表格，stream用来抽取非线框类的表格。 在抽取线框类表格的时候，lattice包含以下几步： 把pdf页面转换成图像 通过图像处理的方式，从页面中检测出水平方向和竖直方向可能用于构成表格的直线。 根据检测出的直线，生成可能表格的bounding box 确定表格各行、列的区域 根据各行、列的区域，水平、竖直方向的表格线以及页面文本内容，解析出表格结构，填充单元格内容，最终形成表格对象。 在抽取线框类表格的时候，stream包含以下几步： 通过pdfminer获取连续字符串 通过文本对齐的方式确定可能表格的bounding box 确定表格各行、列的区域 根据各行、列的区域以及页面上的文本字符串，解析表格结构，填充单元格内容，最终形成表格对象。]]></content>
      <categories>
        <category>表格抽取</category>
      </categories>
      <tags>
        <tag>表格抽取</tag>
        <tag>表格检测</tag>
        <tag>表格识别</tag>
        <tag>开源框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pdfplumber是怎么做表格抽取的（三）]]></title>
    <url>%2F2019%2F12%2F04%2Fpdfplumber-table-extraction-3%2F</url>
    <content type="text"><![CDATA[背景介绍 自底向上的方法 生成单元格 生成表格 存在的问题 背景介绍最近在做一个表格信息抽取的项目，该项目需要从pdf文件中找到的目标表格，并把目标表格中需要的行和列给抽取出来。由于项目中pdf扫描件占比相对较少（不太到10%吧），所以目前主要把精力花在可编辑pdf文件的表格抽取上。 即便是可编辑的pdf文件，从中抽取表格也不是一件容易的事情，概括起来，难在以下几点： 与其说pdf是一种数据格式，不如说它是一组打印指令的集合，因为pdf文件保存的只是一条条打印指令，这些指令告诉pdf阅读器或打印机该在屏幕或者纸张的什么位置显示什么样的符号。与docx和html等格式的文件不同（docx和html通过标签的方式组织不同的逻辑结构，比如&lt;table&gt;, &lt;w:tbl&gt;, &lt;p&gt;, &lt;w:p&gt;等），pdf文件不包含任何逻辑结构的信息，比如段落、句子、单词、表格等等。在pdf文档中，即便在阅读器中能看到table-like的东西，但是却无法直接有效地把这些视觉上table-like的东西所对应的数据给抽取出来。 除了不会保存逻辑结构信息之外，pdf往往也不会保存空格、制表符、回车等不可见字符，所以在pdf中无法像在docx中一样，通过制表符来定位不是用线框表示的表格。 为了从pdf中比较好的抽取表格，作者调研、尝试了许多开源的框架（不限于python开发的框架），包括微软开源的深度学习表格检测与识别模型TableBank。尝试了一圈下来，在基于python的框架中，pdfplumber和camelot的效果相对较好。对于线框完全的表格，二者都能给出比较好的抽取效果，但是对于线框不完全（包含无线框）的表格，二者的效果就差了不少。 因为在项目所需处理的pdf文档中，线框完全及不完全的表格都比较多，所以为了能够理解pdfplumber实现表格抽取的原理和方法，找到改善、提升表格抽取的方法，作者在这里对pdfplubmer的代码逻辑进行了梳理。由于所涉及的内容比较多，所以计划分为三部分进行整理，分别是： pdfplumber是怎么做表格抽取的（一）：介绍pdfplumber及其表格抽取流程 pdfplumber是怎么做表格抽取的（二）：梳理pdfplumber表格线检测逻辑 pdfplumber是怎么做表格抽取的（三）：梳理pdfplumber表格生成逻辑 本文是第三部分。 自底向上的方法在找到了可能的表格线以及这些线的交点之后，接下来就是根据线和交点找到并识别出可能存在的表格。pdfplumber采用了一种自底向上的方式，先根据线和交点找到可能存在的单元格，然后在把连通在一起的单元格组合成一个表格。 生成单元格pdfplumber.table.TableFinder类调用同一模块下的intersections_to_cells函数，根据前面找到的线和交点找出可能存在的单元格。下面是intersections_to_cells函数的代码，据代码所示，生成单元格主要包含以下几步： 首先对所有交点按照自左向右、自上向下的方式排序。 找到以每个交点作为左上角的最小的单元格。因为对输入的交点进行了排序，所以返回的单元格应该也是相同的顺序排序的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def intersections_to_cells(intersections): """ Given a list of points (`intersections`), return all retangular "cells" those points describe. `intersections` should be a dictionary with (x0, top) tuples as keys, and a list of edge objects as values. The edge objects should correspond to the edges that touch the intersection. """ def edge_connects(p1, p2): def edges_to_set(edges): return set(map(utils.obj_to_bbox, edges)) if p1[0] == p2[0]: common = edges_to_set(intersections[p1]["v"])\ .intersection(edges_to_set(intersections[p2]["v"])) if len(common): return True if p1[1] == p2[1]: common = edges_to_set(intersections[p1]["h"])\ .intersection(edges_to_set(intersections[p2]["h"])) if len(common): return True return False points = list(sorted(intersections.keys())) n_points = len(points) def find_smallest_cell(points, i): if i == n_points - 1: return None pt = points[i] rest = points[i+1:] # Get all the points directly below and directly right below = [ x for x in rest if x[0] == pt[0] ] right = [ x for x in rest if x[1] == pt[1] ] for below_pt in below: if not edge_connects(pt, below_pt): continue for right_pt in right: if not edge_connects(pt, right_pt): continue bottom_right = (right_pt[0], below_pt[1]) if ((bottom_right in intersections) and edge_connects(bottom_right, right_pt) and edge_connects(bottom_right, below_pt)): return ( pt[0], pt[1], bottom_right[0], bottom_right[1] ) cell_gen = (find_smallest_cell(points, i) for i in range(len(points))) return list(filter(None, cell_gen)) 生成表格pdfplumber.table.TableFinder类调用同一模块下的cells_to_tables函数，根据前面找到的单元格，把连通的单元格合并到一起生成对应的表格。下面是cells_to_tables函数的代码，需要注意的是，入参cells也是按照自左向右、自上向下排过序的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def cells_to_tables(cells): """ Given a list of bounding boxes (`cells`), return a list of tables that hold those those cells most simply (and contiguously). """ def bbox_to_corners(bbox): x0, top, x1, bottom = bbox return list(itertools.product((x0, x1), (top, bottom))) cells = [ &#123; "available": True, "bbox": bbox, "corners": bbox_to_corners(bbox) &#125; for bbox in cells ] # Iterate through the cells found above, and assign them # to contiguous tables def init_new_table(): return &#123; "corners": set([]), "cells": [] &#125; def assign_cell(cell, table): table["corners"] = table["corners"].union(set(cell["corners"])) table["cells"].append(cell["bbox"]) cell["available"] = False n_cells = len(cells) n_assigned = 0 tables = [] current_table = init_new_table() while True: initial_cell_count = len(current_table["cells"]) for i, cell in enumerate(filter(itemgetter("available"), cells)): if len(current_table["cells"]) == 0: assign_cell(cell, current_table) n_assigned += 1 else: corner_count = sum(c in current_table["corners"] for c in cell["corners"]) if corner_count &gt; 0 and cell["available"]: assign_cell(cell, current_table) n_assigned += 1 if n_assigned == n_cells: break if len(current_table["cells"]) == initial_cell_count: tables.append(current_table) current_table = init_new_table() if len(current_table["cells"]): tables.append(current_table) _sorted = sorted(tables, key=lambda t: min(t["corners"])) filtered = [ t["cells"] for t in _sorted if len(t["cells"]) &gt; 1 ] return filtered 据代码所示，生成表格主要包含以下几步： 对单元格的bbox进行处理，生成四个角的坐标 根据可用单元格四个角的坐标判断单元是否属于当前正在生成的表格。 当单元格与当前正在生成的表格相交时，把该单元格加入到当前表格中，以后该单元格就不再可用了。 当没有单元格可以加入到当前生成的表格的时候，保存该表格，并把当前正在生成的表格设成空表格，判断剩下可用的单元能够加入到当前表格中。 当所有单元格都加入到某一表格之后，停止这一过程。 按照表格的左上角坐标进行排序。 过滤掉那些过小的表格。 把剩下的表格封装到pdfplumber.table.Table类的实例对象，Table类中的extract方法可以根据表格、单元格以及字符的位置，抽取出位于表格及其各个单元格内部的文本，最后以行的形式返回出来。 下面就是Talbe类的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class Table(object): def __init__(self, page, cells): self.page = page self.cells = cells self.bbox = ( min(map(itemgetter(0), cells)), min(map(itemgetter(1), cells)), max(map(itemgetter(2), cells)), max(map(itemgetter(3), cells)), ) @property def rows(self): _sorted = sorted(self.cells, key=itemgetter(1, 0)) xs = list(sorted(set(map(itemgetter(0), self.cells)))) rows = [] for y, row_cells in itertools.groupby(_sorted, itemgetter(1)): xdict = dict((cell[0], cell) for cell in row_cells) row = Row([ xdict.get(x) for x in xs ]) rows.append(row) return rows def extract(self, x_tolerance=utils.DEFAULT_X_TOLERANCE, y_tolerance=utils.DEFAULT_Y_TOLERANCE): chars = self.page.chars table_arr = [] def char_in_bbox(char, bbox): v_mid = (char["top"] + char["bottom"]) / 2 h_mid = (char["x0"] + char["x1"]) / 2 x0, top, x1, bottom = bbox return ( (h_mid &gt;= x0) and (h_mid &lt; x1) and (v_mid &gt;= top) and (v_mid &lt; bottom) ) for row in self.rows: arr = [] row_chars = [ char for char in chars if char_in_bbox(char, row.bbox) ] for cell in row.cells: if cell == None: cell_text = None else: cell_chars = [ char for char in row_chars if char_in_bbox(char, cell) ] if len(cell_chars): cell_text = utils.extract_text(cell_chars, x_tolerance=x_tolerance, y_tolerance=y_tolerance).strip() else: cell_text = "" arr.append(cell_text) table_arr.append(arr) return table_arr 存在的问题最后了，提一下在使用pdfplumber过程中遇到的问题吧，应该会随着使用的深入不断补充，如果不懒的话 ^_^ 在用文本对齐的方式猜测可能存在的不可见的表格线的时候，整个过程是在整个页面上展开的，不会排除那些某一坐标对齐，但是相隔比较远的文本块，这样会导致： 在pdfplumber找到的左对齐、右对齐以及居中对齐的文本块中，某些文本块在竖直距离上相隔比较远，直观上或经验上讲，这些文本块虽然在水平位置上是对齐的，但是却不应该位于同一格表格的某一列中。 好了，都写完啦 ^_^]]></content>
      <categories>
        <category>表格抽取</category>
      </categories>
      <tags>
        <tag>表格抽取</tag>
        <tag>表格检测</tag>
        <tag>表格识别</tag>
        <tag>开源框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pdfplumber是怎么做表格抽取的（二）]]></title>
    <url>%2F2019%2F12%2F03%2Fpdfplumber-table-extraction-2%2F</url>
    <content type="text"><![CDATA[背景介绍 得到定义表格的“边” 看得见的边 看不见的边 额外指定的边 合并找到的边 找到相交的点 背景介绍最近在做一个表格信息抽取的项目，该项目需要从pdf文件中找到的目标表格，并把目标表格中需要的行和列给抽取出来。由于项目中pdf扫描件占比相对较少（不太到10%吧），所以目前主要把精力花在可编辑pdf文件的表格抽取上。 即便是可编辑的pdf文件，从中抽取表格也不是一件容易的事情，概括起来，难在以下几点： 与其说pdf是一种数据格式，不如说它是一组打印指令的集合，因为pdf文件保存的只是一条条打印指令，这些指令告诉pdf阅读器或打印机该在屏幕或者纸张的什么位置显示什么样的符号。与docx和html等格式的文件不同（docx和html通过标签的方式组织不同的逻辑结构，比如&lt;table&gt;, &lt;w:tbl&gt;, &lt;p&gt;, &lt;w:p&gt;等），pdf文件不包含任何逻辑结构的信息，比如段落、句子、单词、表格等等。在pdf文档中，即便在阅读器中能看到table-like的东西，但是却无法直接有效地把这些视觉上table-like的东西所对应的数据给抽取出来。 除了不会保存逻辑结构信息之外，pdf往往也不会保存空格、制表符、回车等不可见字符，所以在pdf中无法像在docx中一样，通过制表符来定位不是用线框表示的表格。 为了从pdf中比较好的抽取表格，作者调研、尝试了许多开源的框架（不限于python开发的框架），包括微软开源的深度学习表格检测与识别模型TableBank。尝试了一圈下来，在基于python的框架中，pdfplumber和camelot的效果相对较好。对于线框完全的表格，二者都能给出比较好的抽取效果，但是对于线框不完全（包含无线框）的表格，二者的效果就差了不少。 因为在项目所需处理的pdf文档中，线框完全及不完全的表格都比较多，所以为了能够理解pdfplumber实现表格抽取的原理和方法，找到改善、提升表格抽取的方法，作者在这里对pdfplubmer的代码逻辑进行了梳理。由于所涉及的内容比较多，所以计划分为三部分进行整理，分别是： pdfplumber是怎么做表格抽取的（一）：介绍pdfplumber及其表格抽取流程 pdfplumber是怎么做表格抽取的（二）：梳理pdfplumber表格线检测逻辑 pdfplumber是怎么做表格抽取的（三）：梳理pdfplumber表格生成逻辑 本文是第二部分。 得到定义表格的“边”pdfplumber用三种不同的方式确定pdf文档中可能存在的表格线，分别是： 把可见的线作为候选表格线，这种方式一般用于抽取线框完全的表格。 根据文本的对齐状态，猜测可能的表格线，这种方式一般用于线框不完全的表格。 额外制定表格线，用于辅助线框不完全表格的抽取。 TableFinder类中的get_edges方法把上述三种不同的方式都包含在内，可以通过配置进行选择，具体如何选择这里就不详细介绍了，感兴趣的读者可以参考pdfplumber自身的配置指引。 看得见的边对线框完全的表格，整个表格和各个单元格的边界都是可以用矩形线框表示和区分开来，所以要检测和解析这类表格，可以先把那些可见的、有可能作为表格线的线找出来。 在pdfplumber中，找出可见的线相对比较简单，因为pdfplumber底层是基于pdfminer的，而pdfminer能够把pdf文档中的水平、竖直的线给解析出来。需要注意的是：1. pdfminer会解析出很多非常短、肉眼基本看不出的线框；2. 可见的线框不能位于图像对象中。 当用pdfplumber.open打开pdf文档后，会通过pdfminer对打开的文档进行解析，每一页解析的结果会保存在pdfplumber.page.Page类的实例对象中。Page类是pdfplumber.container.Container子类，Container类定义了访问chars、rects、edges等基本对象的property，因此可以通过Page实例对象本身方便的访问到对应页面解析出的相关对象。 TableFinder类中的get_edges方法通过utils模块中的filter_edges函数对每一个Page实例对象中的解析出的edges对象进行筛选和过滤，过滤条件包括：方向、最小长度等。 看不见的边对于线框不完全的表格（包括无线框表格），在表格和某些单元格的四周并没有完整的、可见的表格线表示它们的边界和范围。人在检测、识别这类表格的时候，似乎不费吹灰之力，但是对计算机而言，仅靠一堆字符以及它们对应的位置信息，似乎就不是那么得心应手了。 如果仍然要先把确定表格和单元格的表格线找出来的话，那么这个时候是没有从pdf文档中直接解析出的可见线框用的。pdfplumber是怎么应对这种情况的呢？它根据文本的对齐情况猜测出一些水平和竖直的线，这些线被称作“Text Edge”，并利用这些线进一步猜测出表格以及单元格的边界，实现表格抽取的目的。 TableFinder类的get_edges方法通过调用同模块中的words_to_edges_v和words_to_edges_h，根据每一页中解析出的words（word指的应该是由每一行上彼此间距较小的字符合成的连续字符串）的对齐情况，猜测出竖直方向和水平方向上可能存在的线。 下面是words_to_edges_h函数的代码，从中比较容易看出其寻找水平Text Edge的逻辑： 根据words的顶部位置进行聚类，聚类结果应该是把words放到了不同的文本行当中。 筛选掉那些包含word少于word_threshhold的文本行 把剩下文本行的顶部和底部边缘线作为找到的边返回。 1234567891011121314151617181920212223242526272829def words_to_edges_h(words, word_threshold=DEFAULT_MIN_WORDS_HORIZONTAL): """ Find (imaginary) horizontal lines that connect the tops of at least `word_threshold` words. """ by_top = utils.cluster_objects(words, "top", 1) large_clusters = filter(lambda x: len(x) &gt;= word_threshold, by_top) rects = list(map(utils.objects_to_rect, large_clusters)) if len(rects) == 0: return [] min_x0 = min(map(itemgetter("x0"), rects)) max_x1 = max(map(itemgetter("x1"), rects)) edges = [ &#123; "x0": min_x0, "x1": max_x1, "top": r["top"], "bottom": r["top"], "width": max_x1 - min_x0, "orientation": "h" &#125; for r in rects ] + [ &#123; "x0": min_x0, "x1": max_x1, "top": r["bottom"], "bottom": r["bottom"], "width": max_x1 - min_x0, "orientation": "h" &#125; for r in rects ] return edges 因为words_to_edges_v的代码较多，这里就不贴了。其实现逻辑跟words_to_edges_h总体类似，区别主要包含以下几方面： 同时用words的左、右和中心位置进行聚类，把words放到不同的列块中。 对不同对齐方式得到文本列按照包含的word数目进行排序，并删除那些word数目低于word_threshold的列。 去除掉一些有相互重叠的列块 通过最右边的列块确定最右边的边界 把剩下列块的左边界和最右边列块的右边界作为找到的边返回。 额外指定的边对于线框不完全的表格，如果表格检抽取效果不佳，pdfplumber支持在用pdfplumber.page.Page类中的find_tables和extract_tables等方法抽取表格的时候，从外部指定一些水平或竖直的线，以提升表格抽取的效果。 合并找到的边通过上面的方法，可能会找到很多线段，其中存在不少的冗余： 某些平行线之间的垂直距离非常小，需要对它们进行对齐，让他们位于同一条直线上，pdfplumer使用平均位置进行对齐。 对于同一直线上的某些线段，相互之间邻近端点的距离非常小，这种情况，pdfplumber会把它们合并成一个线段。 pdfplumber.table.TableFinder类的get_edges方法会调用同一模块下的merge_edges函数实现上述功能。下面是merge_edges的代码： 1234567891011121314151617181920def merge_edges(edges, snap_tolerance, join_tolerance): """ Using the `snap_edges` and `join_edge_group` methods above, merge a list of edges into a more "seamless" list. """ def get_group(edge): if edge["orientation"] == "h": return ("h", edge["top"]) else: return ("v", edge["x0"]) if snap_tolerance &gt; 0: edges = snap_edges(edges, snap_tolerance) if join_tolerance &gt; 0: _sorted = sorted(edges, key=get_group) edge_groups = itertools.groupby(_sorted, key=get_group) edge_gen = (join_edge_group(items, k[0], join_tolerance) for k, items in edge_groups) edges = list(itertools.chain(*edge_gen)) return edges merge_edges函数分别调用同模块下的snap_edges和join_edge_group函数进行平行线的对齐以及同一直线上线段的合并。 找到相交的点因为文档中的表格以及表格单元格基本上都是矩形的，而矩形是可以由其顶点确定的，所以，在找到那些可能是表格或单元格边界的线之后，接下来是找出它们的交点。下面就是pdfplumber.table模块中edges_to__intersections函数的代码，用于找到水平线与竖直线之间的交点，最终的返回的结果是一个字典，以交点坐标作为key，value中保存的是相交于该交点的线。 12345678910111213141516171819def edges_to_intersections(edges, x_tolerance=1, y_tolerance=1): """ Given a list of edges, return the points at which they intersect within `tolerance` pixels. """ intersections = &#123;&#125; v_edges, h_edges = [ list(filter(lambda x: x["orientation"] == o, edges)) for o in ("v", "h") ] for v in sorted(v_edges, key=itemgetter("x0", "top")): for h in sorted(h_edges, key=itemgetter("top", "x0")): if ((v["top"] &lt;= (h["top"] + y_tolerance)) and (v["bottom"] &gt;= (h["top"] - y_tolerance)) and (v["x0"] &gt;= (h["x0"] - x_tolerance)) and (v["x0"] &lt;= (h["x1"] + x_tolerance))): vertex = (v["x0"], h["top"]) if vertex not in intersections: intersections[vertex] = &#123; "v": [], "h": [] &#125; intersections[vertex]["v"].append(v) intersections[vertex]["h"].append(h) return intersections 好了，这部分就到这里啦 ^_^]]></content>
      <categories>
        <category>表格抽取</category>
      </categories>
      <tags>
        <tag>表格抽取</tag>
        <tag>表格检测</tag>
        <tag>表格识别</tag>
        <tag>开源框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pdfplumber是怎么做表格抽取的（一）]]></title>
    <url>%2F2019%2F12%2F02%2Fpdfplumber-table-extraction-1%2F</url>
    <content type="text"><![CDATA[背景介绍 pdfplumber简介 pdfplumber抽取表格的基本流程 背景介绍最近在做一个表格信息抽取的项目，该项目需要从pdf文件中找到的目标表格，并把目标表格中需要的行和列给抽取出来。由于项目中pdf扫描件占比相对较少（不太到10%吧），所以目前主要把精力花在可编辑pdf文件的表格抽取上。 即便是可编辑的pdf文件，从中抽取表格也不是一件容易的事情，概括起来，难在以下几点： 与其说pdf是一种数据格式，不如说它是一组打印指令的集合，因为pdf文件保存的只是一条条打印指令，这些指令告诉pdf阅读器或打印机该在屏幕或者纸张的什么位置显示什么样的符号。与docx和html等格式的文件不同（docx和html通过标签的方式组织不同的逻辑结构，比如&lt;table&gt;, &lt;w:tbl&gt;, &lt;p&gt;, &lt;w:p&gt;等），pdf文件不包含任何逻辑结构的信息，比如段落、句子、单词、表格等等。在pdf文档中，即便在阅读器中能看到table-like的东西，但是却无法直接有效地把这些视觉上table-like的东西所对应的数据给抽取出来。 除了不会保存逻辑结构信息之外，pdf往往也不会保存空格、制表符、回车等不可见字符，所以在pdf中无法像在docx中一样，通过制表符来定位不是用线框表示的表格。 为了从pdf中比较好的抽取表格，作者调研、尝试了许多开源的框架（不限于python开发的框架），包括微软开源的深度学习表格检测与识别模型TableBank。尝试了一圈下来，在基于python的框架中，pdfplumber和camelot的效果相对较好。对于线框完全的表格，二者都能给出比较好的抽取效果，但是对于线框不完全（包含无线框）的表格，二者的效果就差了不少。 因为在项目所需处理的pdf文档中，线框完全及不完全的表格都比较多，所以为了能够理解pdfplumber实现表格抽取的原理和方法，找到改善、提升表格抽取的方法，作者在这里对pdfplubmer的代码逻辑进行了梳理。由于所涉及的内容比较多，所以计划分为三部分进行整理，分别是： pdfplumber是怎么做表格抽取的（一）：介绍pdfplumber及其表格抽取流程 pdfplumber是怎么做表格抽取的（二）：梳理pdfplumber表格线检测逻辑 pdfplumber是怎么做表格抽取的（三）：梳理pdfplumber表格生成逻辑 本文是第一部分。 pdfplumber简介pdfplumber是一款基于pdfminer，完全由python开发的pdf文档解析库，不仅可以获取每个字符、矩形框、线等对象的具体信息，而且还可以抽取文本和表格。目前pdfplumber仅支持可编辑的pdf文档。 虽然pdfminer也可以对可编辑的pdf文档进行解析，但是比较而言，pdfplumber有以下优势： 二者都可以获取到每个字符、矩形框、线等对象的具体信息，但是pdfplumber在pdfminer的基础上进行了封装和处理，使得到的对象更易于使用，对用户更友好。 二者都能对文本解析，但是pdfminer输出的文本在布局上可能与原文差别比较大，但是pdfplumber抽取出的文本与原文可以有更高的一致性。 pdfplumber实现了表格抽取逻辑，基于最基本的字符、线框等对象的位置信息，定位、识别pdf文档中的表格。 pdfplumber抽取表格的基本流程pdfplumber把表格抽取的功能封装在TableFinder这个类中，在其构造函数__init__中，清晰的定义了表格抽取的基本流程。下面截取了TableFinder类__init__函数部分的代码： 1234567891011121314151617181920212223242526272829303132333435363738class TableFinder(object): """ Given a PDF page, find plausible table structures. Largely borrowed from Anssi Nurminen's master's thesis: http://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/21520/Nurminen.pdf?sequence=3 ... and inspired by Tabula: https://github.com/tabulapdf/tabula-extractor/issues/16 """ def __init__(self, page, settings=&#123;&#125;): for k in settings.keys(): if k not in DEFAULT_TABLE_SETTINGS: raise ValueError("Unrecognized table setting: '&#123;0&#125;'".format( k )) self.page = page self.settings = dict(DEFAULT_TABLE_SETTINGS) self.settings.update(settings) for var, fallback in [ ("text_x_tolerance", "text_tolerance"), ("text_y_tolerance", "text_tolerance"), ("intersection_x_tolerance", "intersection_tolerance"), ("intersection_y_tolerance", "intersection_tolerance"), ]: if self.settings[var] == None: self.settings.update(&#123; var: self.settings[fallback] &#125;) self.edges = self.get_edges() self.intersections = edges_to_intersections( self.edges, self.settings["intersection_x_tolerance"], self.settings["intersection_y_tolerance"], ) self.cells = intersections_to_cells( self.intersections ) self.tables = [ Table(self.page, t) for t in cells_to_tables(self.cells) ] pdfplumber抽取表格主要包含以下几步： 因为表格以及单元格都是存在边界的（由可见或不可见的线表示），所以第一步，pdfplumber是找到可见的或猜测出不可见的候选表格线。 因为表格以及单元格基本上都是定义在一块举行区域内，所以第二步，pdfplumber是根据候选的表格线确定它们的交点。 根据得到的交点，找到它们围成的最小的单元格。 把连通的单元格整合到一起，生成一个检测出的表格对象。 好了，这部分就初步写到这里吧 ^_^]]></content>
      <categories>
        <category>表格抽取</category>
      </categories>
      <tags>
        <tag>表格抽取</tag>
        <tag>表格检测</tag>
        <tag>表格识别</tag>
        <tag>开源框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SHAP VALUES —— 什么影响了你的决定？]]></title>
    <url>%2F2019%2F09%2F01%2Fshap-values%2F</url>
    <content type="text"><![CDATA[这是第四节：SHAP VALUES 用途SHAP值（SHapley Additive exPlanations的缩写）从预测中把每一个特征的影响分解出来。可以把它应用到类似于下面的场景当中: 模型认为银行不应该给某人放贷，但是法律上需要银行给出每一笔拒绝放贷的原因。 医务人员想要确定对不同的病人而言，分别是哪些因素导致他们有患某种疾病的风险，这样就可以因人而异地采取针对性的卫生干预措施，直接处理这些风险因素。 工作原理SHAP值通过与某一特征取基线值时的预测做对比，来解释该特征取某一特定值的影响。 可以继续用排列重要性和部分依赖图中用到的例子进行解释。 我们对一个球队会不会赢得“最佳球员”称号进行了预测。 我们可能会有以下疑问： 预测的结果有多大的程度是由球队进了3个球这一事实影响的？ 但是，如果我们像下面这样重新表述一下的话，那么给出具体、定量的答案还是比较容易的： 预测的结果由多大的程度时由球队进了3个球这一事实影响的，而不是某些基线进球数？ 当然，每个球队都由很多特征，所以，如果我们能回答“进球数”的问题，那么我们也能对其它特征重复这一过程。 SHAP值用一种保证良好性质的的方式做这件事。具体而言，用如下等式对预测进行分解： 1sum(SHAP values for all features) = pred_for_team - pred_for_baseline_values 即，用所有特征的SHAP值的加和来解释为什么预测结果与基线不同。这就允许我们用像下面这样的一幅图来对预测进行分解： 该怎么解释这幅图呢？ 我们预测的结果时0.7，而基准值是0.4979。引起预测增加的特征值是粉色的，它们的长度表示特征影响的程度。引起预测降低的特征值是蓝色的。最大的影源自Goal Scored等于2的时候。但ball possesion的值则对降低预测的值具有比较有意义的影响。 如果把粉色条状图的长度与蓝色条状图的长度相减，差值就等于基准值到预测值之间的距离。 要保证基线值加上每个特征各自影响的和等于预测值的话，在技术上还是有一些复杂度的（这并不像听上去那么直接）。我们不会研究这些细节，因为对于使用这项技术来说，这并不是很关键。这篇博客对此做了比较长篇幅的技术解释。 代码示例这里，我们用Shap库计算SHAP值。 沿用部分依赖图中用到的足球数据。 input: 1234567891011import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierdata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')y = (data['Man of the Match'] == "Yes") # Convert from string "Yes"/"No" to binaryfeature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]X = data[feature_names]train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) 看一下数据集某一行数据上的SHAP值（就随意第5行吧）。在查看SHAP值之前，先看一下最原始的预测值。 input: 123456row_to_show = 5data_for_prediction = val_X.iloc[row_to_show] # use 1 row of data here. Could use multiple rows if desireddata_for_prediction_array = data_for_prediction.values.reshape(1, -1)my_model.predict_proba(data_for_prediction_array) output: 1array([[0.3, 0.7]]) 该球队有70%的可能性赢得这一奖项。 接下来是给上面那条预测计算SHAP值的代码。 1234567import shap # package used to calculate Shap values# Create object that can calculate shap valuesexplainer = shap.TreeExplainer(my_model)# Calculate Shap valuesshap_values = explainer.shap_values(data_for_prediction) 上面的shap_values对象是一个包含两个array的list。第一个array是负向结果（不会获奖）的SHAP值，而第二个array是正向结果（获奖）的SHAP值。通常我们从预测正向结果的角度考虑模型的预测结果，所以我们会拿出正向结果的SHAP值（拿出shap_values[1]）。 直接看原始array很麻烦，但是shap库提供一种不错的结果可视化的方式。 input: 12shap.initjs()shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction) output: 如果仔细观察一下计算SHAP值的代码，就会发现在shap.TreeExplainer(my_model)中涉及到了树。但是SHAP库有用于各种模型的解释器。 shap.DeepExplainer适用于深度学习模型 shap.KernelExplainer 适用于各种模型，但是比其它解释器慢，它给出的是SHAP值的近似值而不是精确值。下面是用KernelExplainer得到类似结果的例子。结果跟上面并不一致，这是因为KernelExplainer`计算的是近似值，但是表达的意思是一样的。 input: 1234# use Kernel SHAP to explain test set predictionsk_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)k_shap_values = k_explainer.shap_values(data_for_prediction)shap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction) output: 练习kaggle小练习]]></content>
      <categories>
        <category>可解释性</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>data mining</tag>
        <tag>可解释性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Partial Dependence Plots —— 部分依赖图]]></title>
    <url>%2F2019%2F08%2F28%2Fpartial-plots%2F</url>
    <content type="text"><![CDATA[这是第三节：Partial Dependence Plots 特征重要性展示的是哪些变量对预测的影响最大，而部分依赖图展示的是特征如何影响模型预测的。 可以用部分依赖图回答一些与下面这些类似的问题： 假如保持其它所有的特征不变，经纬度对房价有什么影响？换句话说，相同大小的房子，在不同的地方价格会有什么差别？ 在两组不同的人群上，模型预测出的健康水平差异是由他们的负债水平引起的，还是另有原因？ 如果你对线性回归或逻辑回归比较熟悉的话，部分依赖图起到的效果跟这些模型里面的参数差不多。但是，与简单模型中的参数相比，复杂模型上的依赖图可以捕捉到更复杂的模式。 工作原理跟排列重要性一样，部分依赖图也是要在拟合出模型之后才能进行计算。 模型是在真实的未加修改的真实数据上进行拟合的。 以足球比赛为例，球队间可能在很多方面都存在着不同。比如传球次数，射门次数，得分数等等。乍看上去，似乎很难梳理出这些特征产生的影响。 为了搞清楚部分依赖图是怎样把每个特征的影响分离出来的，首先我们只看一行数据。比如，这行数据显示的可能是一支占有50%的控球时间，传了100次球，射门了10次，得了1分的球队。 接下来，利用训练好的模型和上面的这一行数据去预测该队斩获最佳球员的概率。但是，我们会多次改变某一特征的数值，从而产生一系列预测结果。比如我们会在把控球时间设成40%的时候，得到一个预测结果，设成50%的时候，得到一个预测结果，设成60%的时候，也得到一个结果，以此类推。以从小到大设定的控球时间为横坐标，以相应的预测输出为纵坐标，我们可以把实验的结果画出来。 代码示例在这里，重点不是建模过程，所以在下面的代码中，不会有过多数据探索和建模的内容。 训练模型 123456789101112import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.tree import DecisionTreeClassifierdata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')y = (data['Man of the Match'] == "Yes") # Convert from string "Yes"/"No" to binaryfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]X = data[feature_names]train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y) 为了节省解释的时间，第一个例子用决策树，如下所示。在实际应用中，你可能会用到更复杂的模型。 决策树结构可视化 12345from sklearn import treeimport graphviztree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names)graphviz.Source(tree_graph) 怎么理解得到的决策树： 非叶子节点顶部的数值表示分支标准 节点最底部的一对数字表示当前节点上正负类样本的数目 可以用PDPBox库来生成部分依赖图。示例如下： 123456789from matplotlib import pyplot as pltfrom pdpbox import pdp, get_dataset, info_plots# Create the data that we will plotpdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')# plot itpdp.pdp_plot(pdp_goals, 'Goal Scored')plt.show() 在看上面的部分依赖图的时候，有两点值得注意的地方： y轴表示的是模型预测相较于基线值或最左边的值的变化。 蓝色阴影部分表示置信区间。 从这幅图可以看出，进一个球会显著地增加获得最佳球员称号地机会，但是进更多的球似乎对预测的影响不大。 下面是另外一个示例图： 12345feature_to_plot = 'Distance Covered (Kms)'pdp_dist = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)pdp.pdp_plot(pdp_dist, feature_to_plot)plt.show() 这种图似乎太简单了，并不能代表现实情况。其实这是因为模型太简单了。从上面的的决策树结构图可以看出，上面两幅部分依赖图展示的结果正是决策树的结构。 通过部分依赖图，可以比较轻松地比较不同模型的结构或含义。下面是一个随机森林的例子： 1234567# Build Random Forest modelrf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)pdp_dist = pdp.pdp_isolate(model=rf_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)pdp.pdp_plot(pdp_dist, feature_to_plot)plt.show() 这个模型认为在比赛过程中，如果所有球员一共跑动了100km的话，球队会更有可能斩获最佳球员。但是跑动得更多的话，可能性就会下降一些。 一般来说，这条曲线的光滑形态看上去比决策树的阶跃函数更可信。但是因为例子中用的数据集太小了，所以在对任意一个模型进行解释的时候，要特别注意选用的方式。 2D 部分依赖图如果你对特征之间的相互作用感兴趣的话，2D部分依赖图就能排得上用场了。举个例子看下。 仍然以决策树模型为例，下面会生成一个非常简单的图，但是你仍然能够把你从图中看出的东西跟决策树本身的结果匹配到一起。 123456# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plotfeatures_to_plot = ['Goal Scored', 'Distance Covered (Kms)']inter1 = pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour', plot_pdp=True)plt.show() 上面这幅图展示了在Goals Scored和Distance coverd两个特征的任意组合上的预测结果。 比如，当一个球队得了至少一分，并且跑通的总距离接近100km的时候，模型预测的结果最高。如果得了0分，跑动的距离就没什么用了。你能从决策树中看出这一结果吗？ 但是如果球队得分了的话，跑动的距离是会影响到预测的。确定你可以从2D的部分依赖图中看出这一个结果。你能从决策树中看出同样的结果吗? 练习kaggle小练习]]></content>
      <categories>
        <category>可解释性</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>data mining</tag>
        <tag>可解释性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Permutaion Importance —— 排列重要性]]></title>
    <url>%2F2019%2F08%2F17%2Fpermutaion-importance%2F</url>
    <content type="text"><![CDATA[转载请注明来源：http://iceflameworm.github.io/2019/08/17/permutation-importance/ 在推广数据分析、挖掘以及模型训练结果的时候，经常遇到客户或业务方需要我们对其进行解读。如果不能让客户或业务方很好地理解或者接受的话，“数据民工们”的工作成果就很难被有效地推行下去，哎，宝宝们都很苦啊。 最近忽然想起之前在逛kaggle的时候看到过有模型解释性相关的课程，于是就回头温习下，梳理下要点，一来加深自己的理解，也方便感兴趣的同学阅读。 这是第二节：Permutaion Importance 作用与特点训练得到一个模型之后，我们可能会问的一个最基本的问题是 哪些特征对预测结果的影响最大？ 这一概念叫做 特征重要性。 有很多度量特征重要性的方法。一些方法回答的问题与上述问题略有不同，而另外一些方法则具有一些documented shortcomings (暂译为：记录的缺点)。 与其它方法相比，排列重要性具有以下优点： 计算速度快 应用广泛、易于理解 与我们期望一个特征重要性度量所具有的性质一致 工作原理排列重要性使用模型的方式与你迄今为止所见到过的都不同，而且在一开始，很多人都会对其感到很困惑。所以首先举一个例子来具体介绍以下它。 假定有以下格式的数据集： 我们想用一个人10岁的数据去预测他20岁的身高是多少？ 数据中包含： 有用的特征（10岁时的身高） 较弱的特征（10岁时拥有的股票） 对预测基本没有作用的特征 排列重要性是要在模型拟合之后才能进行计算。 所以对于给定的身高、股票数量等取值之后，计算排列重要性并不会改变模型或者是它的预测结果。 相反，我们会问以下问题：如果随机打乱验证数据某一列的值，保持目标列以及其它列的数据不变，那么这种操作会在这些打乱的数据上对预测准确率产生怎样的影响？ 对某一列进行随机排序应当会降低预测的准确率，这是因为产生的数据不再对应于现实世界中的任何东西。如果随机打乱的那一列模型预测对其依赖程度很高，那么模型准确率的衰减程度就会更大。在这个例子中，打乱height at age 10将会让预测结果非常差。但是如果我们随机打乱的是socks owned，那么产生的预测结果就不会衰减得那么厉害。 有了上述认识之后，排列重要性就按照以下步骤进行计算： 得到一个训练好的模型 打乱某一列数据的值，然后在得到的数据集上进行预测。用预测值和真实的目标值计算损失函数因为随机排序升高了多少。模型性能的衰减量代表了打乱顺序的那一列的重要程度。 将打乱的那一列复原，在下一列数据上重复第2步操作，直到计算出了每一列的重要性。 代码示例下面的例子会用到这样一个模型，这个模型用球队的统计数据预测一个足球队会不会出现“全场最佳球员”。“全场最佳球员”奖是颁发给比赛里表现最好的球员的。我们现在关注的并不是建模的过程，所以下面的代码只是载入了数据，然后构建了一个很基础的模型。 1234567891011import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierdata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')y = (data['Man of the Match'] == "Yes") # Convert from string "Yes"/"No" to binaryfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]X = data[feature_names]train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) 下面演示如何用eli5库计算和展示排列重要性。 12345import eli5from eli5.sklearn import PermutationImportanceperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)eli5.show_weights(perm, feature_names = val_X.columns.tolist()) 输出结果： 排列重要性结果解读排在最上面的是最重要的特征，排在最下面是重要性最低的特征。 每一行的第一个数字表示模型性能衰减了多少（在这个例子中，使用准确率作为性能度量）。 跟数据科学里面的很多事情一样，在对某一打乱的特征提取重要性的时候，是存在随机性的，所以我们在计算排列重要性的时候，会通过多次打乱顺序的方式重复这一过程。在&plusmn;后面的数字表示标准差。 偶尔你会看到负值的排列重要性。在这些情况中，在打乱的数据上得到预测结果比真实数据的准确率更高。这在所选特征与目标基本无关（重要性应该为0）的情况下会出现，但是随机的因素导致预测结果在打乱的数据上表现得更准确。就像这个例子一样，因为没有容忍随机性的空间，这种情况在小的数据集上很常见。 在我们的例子中，最重要的特征是Goals scored，看上去似乎是说得通的。对于其他变量的排序是否令人意外，足球球迷可能会有比较直观的感觉。 练习kaggle小练习]]></content>
      <categories>
        <category>可解释性</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>data mining</tag>
        <tag>可解释性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Use cases for model insights —— 模型可解释性的应用场景]]></title>
    <url>%2F2019%2F08%2F15%2Fuse-cases-of-model-insights%2F</url>
    <content type="text"><![CDATA[转载请注明来源：http://iceflameworm.github.io/2019/08/15/use-cases-of-model-insights/ 在推广数据分析、挖掘以及模型训练结果的时候，经常遇到客户或业务方需要我们对其进行解读。如果不能让客户或业务方很好地理解或者接受的话，“数据民工们”的工作成果就很难被有效地推行下去，哎，宝宝们都很苦啊。 最近忽然想起之前在逛kaggle的时候看到过有模型解释性相关的课程，于是就回头温习下，梳理下要点，一来加深自己的理解，也方便感兴趣的同学阅读。 这是第一节：Use cases for model insights 做什么？很多人把各种机器学习模型看作是黑盒子，因为虽然这些模型给出的预测结果还不错，但是你却无法理解这些预测背后的逻辑。 要加深对模型结果的理解，可以从以下几个方面入手 训练出的模型认为哪些特征很重要？ 对于任意一个数据样本，每一个特征是如何影响其预测结果的。 从宏观意义上讲，每个特征是如何影响模型在整体上的预测结果的。 应用场景对模型结果的解释可应用到以下这些场景当中： 模型调试 特征工程 数据收集 决策制定 建立信任 模型调试在实际应用场景中，往往会存在很多不可靠、组织混乱和存在污染的数据。在对数据进行预处理的时候，很有可能在不经意间就会引入潜在的错误源。在实际的数据科学项目中，在某些地方出现错误是很正常的事情。 因为bug会频繁出现，且可能会引起灾难性的后果，所以模型调式数据科学领域最重要的技能之一。理解模型发现的模式会帮你确定什么时候它们跟你所掌握的知识不一致，而这通常才是追踪bug原因第一步而已。 特征工程一般情况下，特征工程是提升模型准确率最有效的方法。通常，特征工程需要不断地在原有数据或已创建的特征上进行变换，来产生新的特征。 在数据量、维度都比较小的情况下，有时你只靠自身对问题的直觉，就可以完成这一过程。但是当你需要处理成百上千的原始特征，或者并不太了解问题的背景的时候，你就需要更多的指导和建议了。 数据收集对于从网上下载的数据集，你是无法控制的。但是很多使用数据科学的企业和组织都有机会扩展所收集数据的类型。因为收集新类型的数据可能成本会很高，或者非常麻烦，所以只有在清楚这么做是划算的时候，企业和组织才会去做。基于模型的解释会帮你更好地理解现有特征地价值，进而推断出哪些新数据可能是最有帮助的。 决策制定某些情况下，模型会直接自动做出决策，但是有很多重要的决策是需要人来确定。对于最终需要人来做的决策，模型的可解释性比单纯的预测结果更重要。 建立信任许多人在确定一些基本的事实之前，不会信赖你用来做重要决策的模型。鉴于数据错误的频繁出现，这是一种明智的防范措施。在实际业务场景中，如果给出的模型解释符合对方自身对问题的理解，那么即使在基本不具备深入的数据科学知识的人之间，也将有助于建立互相信任的关系。]]></content>
      <categories>
        <category>可解释性</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>data mining</tag>
        <tag>可解释性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源框架gunthercox/ChatterBot浅析]]></title>
    <url>%2F2019%2F08%2F12%2Fchatterbot-github%2F</url>
    <content type="text"><![CDATA[转载请注明来源：http://iceflameworm.github.io/2019/08/12/chatterbot-github/ 框架简介Chatterbot是一个完全用python编写的基于文本检索/匹配的聊天机器人框架，它会从保存的对话语料中找出与输入句子最匹配的句子，并把匹配到的句子的下一句作为回答返回。本文主要对其工作流程，以及核心的训练器和逻辑适配器进行梳理，具体使用方法，请参考其文档。 框架地址：https://github.com/gunthercox/ChatterBot文档地址：https://chatterbot.readthedocs.io/en/stable/ 工作流程 原文档中有两幅描述工作流程的示意图，一幅在文档首页，一幅在文档-逻辑适配器，个人认为后者描述的更全面、更恰当些，所以就以后者为准进行介绍。 从输入句子到输出响应回答，前后需要经历三大步： 预处理 生成答案 答案选择 具体流程请参考chatterbot.py。 预处理与通常所讲的NLP预处理的目的基本一致，主要是文本进行一些标准还操作，比如去除连续的空格、删除特殊字符等等。Chatterbot框架自身实现了clean_whitespace, unescape_html和convert_to_ascii三种预处理功能。具体实现参见preprocessors.py 生成答案一个Chatterbot实例可以绑定多个逻辑适配器，用于根据输入产生输出。 Chatterbot中没有独立的用于选择对话逻辑的意图识别模块，它将意图识别的功能放到了各个逻辑适配器中。接收到输入之后，Chatterbot会将其传递给各个逻辑适配器，由它们自己判断是否适合对输入的文本进行回答。如果逻辑适配器认为不能对输入进行回答，则会跳过，否则就输出回答，这样的话，有可能所有逻辑适配器都不输出回答，也有可能有多个逻辑适配器都给出了回答。具体请参考：ChatBot::generate_response方法。 从ChatBot::__init__和Chatbot::generate_response中的两端代码 12345678910# __init__self.storage = utils.initialize_class(storage_adapter, **kwargs)...# generate_responsefor adapter in logic_adapters: utils.validate_adapter_class(adapter, LogicAdapter) logic_adapter = utils.initialize_class(adapter, self, **kwargs) self.logic_adapters.append(logic_adapter) 可知，所有的逻辑适配器都共享一份保存的对话语料。倘若逻辑适配器内部不对数据进行选择的话，所有的逻辑适配器都将从所有的对话语料数据中查找最匹配的回答。这样的结果就是，每个逻辑适配器在相同的数据上用不同的匹配方法或指标产生回答，衡量每个回答confidence的标准并不一致，这会影响后续根据confidence进行答案选择。 答案选择工作流程示意图显示，Chatterbot会从所有的逻辑适配器返回的回答中，选择confidence最大的，但是ChatBot::generate_response实现的是另外一种逻辑。在Chatbot::generate_response中，每个逻辑适配器输出的confidence并没有用到，它会统计每一个返回的回答出现的次数，如果有出现次数大于1次的，则会返回出现次数最多的回答，但是如果所有逻辑适配器返回的回答都只出现了一次，则会第一个逻辑适配器的答案，个人认为这种逻辑存在缺陷。以下是相关代码： 123456789101112131415161718192021222324# If multiple adapters agree on the same statement,# then that statement is more likely to be the correct responseif len(results) &gt;= 3: result_options = &#123;&#125; for result_option in results: result_string = result_option.text + ':' + (result_option.in_response_to or '') if result_string in result_options: result_options[result_string].count += 1 if result_options[result_string].statement.confidence &lt; result_option.confidence: result_options[result_string].statement = result_option else: result_options[result_string] = ResultOption( result_option ) most_common = list(result_options.values())[0] for result_option in result_options.values(): if result_option.count &gt; most_common.count: most_common = result_option if most_common.count &gt; 1: result = most_common.statement 训练器刚开始的时候，以为这里的训练跟普通的训练是一样的，也就是通过数据+训练过程确定模型的参数。实际上，这里的训练过程不能算作真正的训练，有点跟KNN算法的训练过程差不多（KNN也没有真正意义上的训练过程），所谓的训练过程其实就是准备检索数据的过程。结合文档示例和 ListTrainer 可以看出Chatterbot框架中的训练过程实际上是怎样的。 文档中的示例 1234567891011121314from chatbot import chatbotfrom chatterbot.trainers import ListTrainertrainer = ListTrainer(chatbot)trainer.train([ "Hi there!", "Hello",])trainer.train([ "Greetings!", "Hello",]) ListTrainer 1234567891011121314151617181920212223242526272829303132333435363738394041class ListTrainer(Trainer): """ Allows a chat bot to be trained using a list of strings where the list represents a conversation. """ def train(self, conversation): """ Train the chat bot based on the provided list of statements that represents a single conversation. """ previous_statement_text = None previous_statement_search_text = '' statements_to_create = [] for conversation_count, text in enumerate(conversation): if self.show_training_progress: utils.print_progress_bar( 'List Trainer', conversation_count + 1, len(conversation) ) statement_search_text = self.chatbot.storage.tagger.get_text_index_string(text) statement = self.get_preprocessed_statement( Statement( text=text, search_text=statement_search_text, in_response_to=previous_statement_text, search_in_response_to=previous_statement_search_text, conversation='training' ) ) previous_statement_text = statement.text previous_statement_search_text = statement_search_text statements_to_create.append(statement) self.chatbot.storage.create_many(statements_to_create) 对话语料的保存并不是以文本对为单位的，而是把每一句话作为最基本的存储单元，语句之间的关系通过*in_response_to字段表示。如下述代码所示，Statement本身的text是用来回答in_response_to对应的previous_statement_text这句话的。 1234567Statement( text=text, search_text=statement_search_text, in_response_to=previous_statement_text, search_in_response_to=previous_statement_search_text, conversation='training') 逻辑适配器逻辑适配器主要用于根据输入文本产生相应的回答。Chatterbot本身实现了一个BestMatch的逻辑适配器，它会从保存的对话语料中找出与输入文本最匹配的回答，其基本检索流程是： 根据某一相似度度量指标，从保存的对话语料中找到与输入文本最相似的文本 mtext。 遍历整个对话语料，找出所有可以用来回答m_text的文本。 从上述步骤可以看出， BestMatch找到回答需要遍历两次保存的对话语料，在实现方式上可能不是最优的。 具体流程参考 BestMatch::process。 在文本相似度度量上，Chatterbot本身已经实现了多种不同的方法： LevenshteinDistance-编辑距离 JaccardSimilarity 使用Spacy计算的相似度 具体请参考 comparisons.py。]]></content>
      <categories>
        <category>chatbot</category>
      </categories>
      <tags>
        <tag>开源框架</tag>
        <tag>NLP</tag>
        <tag>chatbot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[闻言、明义、知心——你的智能伙伴，聊天机器人]]></title>
    <url>%2F2019%2F08%2F08%2Fchatbot-wechat%2F</url>
    <content type="text"><![CDATA[第一篇真正的博文，就转发一下自己为公司写的软文吧，闻言、明义、知心——你的智能伙伴，聊天机器人 “Hello！I am Baymax，Your personal healthcare companion.” （“你好，我是大白，你的个人健康伙伴”）——《超能陆战队》。 《超能陆战队》是一部于2015年上映的动漫科幻电影，成功塑造了“大白”这样一个憨态可掬、善解人意的智能机器人的形象。与冷冰冰的机器相比，大白不仅能跟主人公小宏进行萌萌哒的语言交流，而且还能感受、理解小宏的意图、情绪和情感，帮助小宏克服种种困难，完成各种挑战。假如现在也给我们这样一个“大白”的话，我想你我应该都不会拒绝的。 虽然到目前为止， 现实中还不存在像“大白”这样一个暖心的智能伙伴，但是人工智能技术地发展，催生了各种聊天机器人的出现（小冰、小娜、小蜜、小爱同学、Siri、balabala…），正让这位暖心的朋友一步一步地向我们走来。现在这些还活在我们手机、电脑或音箱里的聊天机器人已经可以帮我们做一些类似于机票预定、问题解答等相关的事情了，甚至它们还会不时地“撩”你一下哟。 环肥燕瘦 自2016年谷歌I/O开发者大会发布Google Home智能音箱以来，国内外掀起了一股聊天机器人产品化的热潮，国内外的百度、阿里、小米、亚马逊、微软等知名厂商纷纷发布了各自的聊天机器人产品和开放平台。微软的当家人萨提亚·纳德拉更是提出了对话即平台（Conversations as a Platform， CaaS）的发展战略。 现在已经迈入2019年下半年的门槛了，到目前为止，市面上涌现了各式各样的聊天机器人产品。这些聊天机器人可能是一个圆圆或者方方的智能音箱，安静的待在角落里时刻等待着你的召唤。它们还可能藏身于一部手机、一块手表，时刻陪在你的身边，帮你订外卖、问天气、查路线。有的时候它们会化身为“人”，热情地欢迎你的光临。或许你不是一个张扬的人，不喜欢把一切都说出来，它们也会耐心地跟你用文字传递心意。 多才多艺 各种各样的聊天机器人产品正通过语言和文字这两种最接近人类交互的方式，慢慢地融入到我们生活、学习、工作当中，让我们身边的一切都开始变得智能起来。目前，聊天机器人已经在智能家居、智能生活、智能客服等领域取得了初步的成效。未来，在更多的领域，它们都会像一位专属于你的私人秘书一样，只要你跟他简单地“说”一句，它们就会理解你的想法，帮你完成想做的事情。 智能家居 目前很多的智能音箱产品或者语音助手都提供了家居场景的解决方案，用户可以通过人机对话的方式对家庭中各类电子设备的控制，如家居机器人、灯光、冰箱、插座、空调、电视、窗帘等等，彻底解放双手。想象一下，未来在自己的家中，我们或许真的能过上“衣来伸手饭来张口”的日子。 智能生活 现在是移动互联网的时代，智能手机与各种智能穿戴设备都已经非常流行。在系统级别上，用户在唤醒手机助手或智能设备后，可以通过对话的方式进行交流、操作和控制，获取资讯以及各项生活服务 等等。在app级别上，用户亦可以通过对话的方式，免手持对app进行功能操作或内容获取等。现在我们已经可以通过语音助手查天气、订外卖、订机票、打电话、发短信等等一系列事情了，未来，随着技术的不断进步，我们可能会拥有一位更全能的智能“生活管家”。 智能客服 现在各种网站以及app开始越来越多的把聊天机器人集成到客服系统中，相对于人工座席客服，聊天机器人可以提供7*24不间断的服务。目前机器人客服已经可以代替人工客服解答一些高频的问题，使得人工客服能够聚焦于更高价值的业务，有效地降低了人力成本，提升了人效。 其它 除了上述应用场景外，聊天机器人还被越来越多地应用于智能出行、智能服务等场景中，随着人工智能的不断发展，我们可能会在越来越多的场景中看到聊天机器人的身影。 三大流派聊天机器人的形态多种多样，交互的方式也不尽相同，但是对所包含的功能进行划分的话，总体上可以分为任务型、问答型和闲聊型三种。有些机器人产品只包含某一种类型的功能，而另外一些则会集成其中两种或所有类型的功能。 任务型任务型机器人指特定条件下提供信息或服务的机器人。通常情况下是为了满足带有明确目的的用户，例如查流量，查话费，订餐，订票，咨询等任务型场景。由于用户的需求较为复杂，通常情况下需分多轮互动，用户也可能在对话过程中不断修改与完善自己的需求，任务型机器人需要通过询问、澄清和确认来帮助用户明确目的。其基本流程如下： 口语理解（SLU）：把用户输入的自然语言转变为结构化信息——act-slot-value三元组。例如餐厅订座应用中用户说“订云海肴中关村店”，我们通过NLU把它转化为结构化信息：“inform(order_op=预订, restaurant_name=云海肴, subbranch=中关村店)”，其中的“inform”是动作名称，而括号中的是识别出的槽位及其取值。 对话管理（DM）：综合用户当前query和历史对话中已获得的信息后，给出机器答复的结构化表示。对话管理包含两个模块：对话状态追踪（DST）和策略优化（DPO）。 DST维护对话状态，它依据最新的系统和用户行为，把旧对话状态更新为新对话状态。其中对话状态应该包含持续对话所需要的各种信息。 DPO根据DST维护的对话状态，确定当前状态下机器人应如何进行答复，也即采取何种策略答复是最优的。 自然语言产生（NLG）：把DM输出的结构化对话策略还原成对人友好的自然语言。简单的NLG方法可以是事先设定好的回复模板，复杂的可以使用深度学习生成模型。 问答型问答型机器人的主要任务是从特定知识库中找出与用户提出的问题最匹配的答案。智能客服通常都会包含这一类型的机器人，用于回答一些高频的问题，减轻人工客服的压力。其基本流程如下： 文本检索和匹配是问答型机器人的核心组成部分，除此之外，可能还会重排序这一功能模块。 文本检索：首先通过检索的方式从知识库的大量语料中筛选出可能包含答案的候选集，供后续匹配模块打分，计算相似度。文本检索主要包含词语级别和句子级别两种方式，其中词语级别是通过关键词匹配进行的，句子级别是基于语义度量进行的。 文本匹配：通过字面或者语义等不同的方式计算用户所提问题与候选集的相似度。这些相似度会直接用于返回答案或者供后续重排序模块使用。 重排序：类似于ensemble learning中的stacking，利用各种不同类型的相似度分数作为输入，重新对候选集进行排序，排序的结果将最终用于生成答案。 闲聊型真实应用中，用户与系统交互的过程中不免会涉及到闲聊成分。闲聊功能可以让对话机器人更有情感和温度。闲聊机器人可以通过事先准备的闲聊库实现，这样的话，类似于问答型的机器人。另外一种实现方式是使用机器翻译中的深度学习seq2seq框架来产生答复，相比于前一种实现方式，这种方式通常能产生更多样化的回答，智能化程度更高。 结语聊天机器人承载了全新的交互形式，可能带来了全新的产品服务体验，这种进步过去从来没有过。我们每个人几乎都在渴望着机器人时代的到来，也许很遥远，也许已经在路上，谁知道呢？Hello，大白，你快到了吗？ 参考资料 小米AI音箱：https://www.mi.com/aispeaker/ 天猫精灵：https://bot.tmall.com/ unit开放平台：https://ai.baidu.com/unit/home 关于对话机器人，你需要了解这些技术：https://blog.csdn.net/qq_40027052/article/details/78723576 揭秘任务型对话机器人（上篇）：https://www.cnblogs.com/qcloud1001/p/9181900.html AnyQ：https://github.com/baidu/AnyQ]]></content>
      <categories>
        <category>chatbot</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>chatbot</tag>
      </tags>
  </entry>
</search>

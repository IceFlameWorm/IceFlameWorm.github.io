<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SHAP VALUES —— 什么影响了你的决定？]]></title>
    <url>%2F2019%2F09%2F01%2Fshap-values%2F</url>
    <content type="text"><![CDATA[这是第四节：SHAP VALUES 用途SHAP值（SHapley Additive exPlanations的缩写）从预测中把每一个特征的影响分解出来。可以把它应用到类似于下面的场景当中: 模型认为银行不应该给某人放贷，但是法律上需要银行给出每一笔拒绝放贷的原因。 医务人员想要确定对不同的病人而言，分别是哪些因素导致他们有患某种疾病的风险，这样就可以因人而异地采取针对性的卫生干预措施，直接处理这些风险因素。 工作原理SHAP值通过与某一特征取基线值时的预测做对比，来解释该特征取某一特定值的影响。 可以继续用排列重要性和部分依赖图中用到的例子进行解释。 我们对一个球队会不会赢得“最佳球员”称号进行了预测。 我们可能会有以下疑问： 预测的结果有多大的程度是由球队进了3个球这一事实影响的？ 但是，如果我们像下面这样重新表述一下的话，那么给出具体、定量的答案还是比较容易的： 预测的结果由多大的程度时由球队进了3个球这一事实影响的，而不是某些基线进球数？ 当然，每个球队都由很多特征，所以，如果我们能回答“进球数”的问题，那么我们也能对其它特征重复这一过程。 SHAP值用一种保证良好性质的的方式做这件事。具体而言，用如下等式对预测进行分解： 1sum(SHAP values for all features) = pred_for_team - pred_for_baseline_values 即，用所有特征的SHAP值的加和来解释为什么预测结果与基线不同。这就允许我们用像下面这样的一幅图来对预测进行分解： 该怎么解释这幅图呢？ 我们预测的结果时0.7，而基准值是0.4979。引起预测增加的特征值是粉色的，它们的长度表示特征影响的程度。引起预测降低的特征值是蓝色的。最大的影源自Goal Scored等于2的时候。但ball possesion的值则对降低预测的值具有比较有意义的影响。 如果把粉色条状图的长度与蓝色条状图的长度相减，差值就等于基准值到预测值之间的距离。 要保证基线值加上每个特征各自影响的和等于预测值的话，在技术上还是有一些复杂度的（这并不像听上去那么直接）。我们不会研究这些细节，因为对于使用这项技术来说，这并不是很关键。这篇博客对此做了比较长篇幅的技术解释。 代码示例这里，我们用Shap库计算SHAP值。 沿用部分依赖图中用到的足球数据。 input: 1234567891011import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierdata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')y = (data['Man of the Match'] == "Yes") # Convert from string "Yes"/"No" to binaryfeature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]X = data[feature_names]train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) 看一下数据集某一行数据上的SHAP值（就随意第5行吧）。在查看SHAP值之前，先看一下最原始的预测值。 input: 123456row_to_show = 5data_for_prediction = val_X.iloc[row_to_show] # use 1 row of data here. Could use multiple rows if desireddata_for_prediction_array = data_for_prediction.values.reshape(1, -1)my_model.predict_proba(data_for_prediction_array) output: 1array([[0.3, 0.7]]) 该球队有70%的可能性赢得这一奖项。 接下来是给上面那条预测计算SHAP值的代码。 1234567import shap # package used to calculate Shap values# Create object that can calculate shap valuesexplainer = shap.TreeExplainer(my_model)# Calculate Shap valuesshap_values = explainer.shap_values(data_for_prediction) 上面的shap_values对象是一个包含两个array的list。第一个array是负向结果（不会获奖）的SHAP值，而第二个array是正向结果（获奖）的SHAP值。通常我们从预测正向结果的角度考虑模型的预测结果，所以我们会拿出正向结果的SHAP值（拿出shap_values[1]）。 直接看原始array很麻烦，但是shap库提供一种不错的结果可视化的方式。 input: 12shap.initjs()shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction) output: 如果仔细观察一下计算SHAP值的代码，就会发现在shap.TreeExplainer(my_model)中涉及到了树。但是SHAP库有用于各种模型的解释器。 shap.DeepExplainer适用于深度学习模型 shap.KernelExplainer 适用于各种模型，但是比其它解释器慢，它给出的是SHAP值的近似值而不是精确值。下面是用KernelExplainer得到类似结果的例子。结果跟上面并不一致，这是因为KernelExplainer`计算的是近似值，但是表达的意思是一样的。 input: 1234# use Kernel SHAP to explain test set predictionsk_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)k_shap_values = k_explainer.shap_values(data_for_prediction)shap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction) output: 练习kaggle小练习]]></content>
      <categories>
        <category>可解释性</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>data mining</tag>
        <tag>可解释性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Partial Dependence Plots —— 部分依赖图]]></title>
    <url>%2F2019%2F08%2F28%2Fpartial-plots%2F</url>
    <content type="text"><![CDATA[这是第三节：Partial Dependence Plots 特征重要性展示的是哪些变量对预测的影响最大，而部分依赖图展示的是特征如何影响模型预测的。 可以用部分依赖图回答一些与下面这些类似的问题： 假如保持其它所有的特征不变，经纬度对房价有什么影响？换句话说，相同大小的房子，在不同的地方价格会有什么差别？ 在两组不同的人群上，模型预测出的健康水平差异是由他们的负债水平引起的，还是另有原因？ 如果你对线性回归或逻辑回归比较熟悉的话，部分依赖图起到的效果跟这些模型里面的参数差不多。但是，与简单模型中的参数相比，复杂模型上的依赖图可以捕捉到更复杂的模式。 工作原理跟排列重要性一样，部分依赖图也是要在拟合出模型之后才能进行计算。 模型是在真实的未加修改的真实数据上进行拟合的。 以足球比赛为例，球队间可能在很多方面都存在着不同。比如传球次数，射门次数，得分数等等。乍看上去，似乎很难梳理出这些特征产生的影响。 为了搞清楚部分依赖图是怎样把每个特征的影响分离出来的，首先我们只看一行数据。比如，这行数据显示的可能是一支占有50%的控球时间，传了100次球，射门了10次，得了1分的球队。 接下来，利用训练好的模型和上面的这一行数据去预测该队斩获最佳球员的概率。但是，我们会多次改变某一特征的数值，从而产生一系列预测结果。比如我们会在把控球时间设成40%的时候，得到一个预测结果，设成50%的时候，得到一个预测结果，设成60%的时候，也得到一个结果，以此类推。以从小到大设定的控球时间为横坐标，以相应的预测输出为纵坐标，我们可以把实验的结果画出来。 代码示例在这里，重点不是建模过程，所以在下面的代码中，不会有过多数据探索和建模的内容。 训练模型 123456789101112import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.tree import DecisionTreeClassifierdata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')y = (data['Man of the Match'] == "Yes") # Convert from string "Yes"/"No" to binaryfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]X = data[feature_names]train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y) 为了节省解释的时间，第一个例子用决策树，如下所示。在实际应用中，你可能会用到更复杂的模型。 决策树结构可视化 12345from sklearn import treeimport graphviztree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names)graphviz.Source(tree_graph) 怎么理解得到的决策树： 非叶子节点顶部的数值表示分支标准 节点最底部的一对数字表示当前节点上正负类样本的数目 可以用PDPBox库来生成部分依赖图。示例如下： 123456789from matplotlib import pyplot as pltfrom pdpbox import pdp, get_dataset, info_plots# Create the data that we will plotpdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')# plot itpdp.pdp_plot(pdp_goals, 'Goal Scored')plt.show() 在看上面的部分依赖图的时候，有两点值得注意的地方： y轴表示的是模型预测相较于基线值或最左边的值的变化。 蓝色阴影部分表示置信区间。 从这幅图可以看出，进一个球会显著地增加获得最佳球员称号地机会，但是进更多的球似乎对预测的影响不大。 下面是另外一个示例图： 12345feature_to_plot = 'Distance Covered (Kms)'pdp_dist = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)pdp.pdp_plot(pdp_dist, feature_to_plot)plt.show() 这种图似乎太简单了，并不能代表现实情况。其实这是因为模型太简单了。从上面的的决策树结构图可以看出，上面两幅部分依赖图展示的结果正是决策树的结构。 通过部分依赖图，可以比较轻松地比较不同模型的结构或含义。下面是一个随机森林的例子： 1234567# Build Random Forest modelrf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)pdp_dist = pdp.pdp_isolate(model=rf_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)pdp.pdp_plot(pdp_dist, feature_to_plot)plt.show() 这个模型认为在比赛过程中，如果所有球员一共跑动了100km的话，球队会更有可能斩获最佳球员。但是跑动得更多的话，可能性就会下降一些。 一般来说，这条曲线的光滑形态看上去比决策树的阶跃函数更可信。但是因为例子中用的数据集太小了，所以在对任意一个模型进行解释的时候，要特别注意选用的方式。 2D 部分依赖图如果你对特征之间的相互作用感兴趣的话，2D部分依赖图就能排得上用场了。举个例子看下。 仍然以决策树模型为例，下面会生成一个非常简单的图，但是你仍然能够把你从图中看出的东西跟决策树本身的结果匹配到一起。 123456# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plotfeatures_to_plot = ['Goal Scored', 'Distance Covered (Kms)']inter1 = pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour', plot_pdp=True)plt.show() 上面这幅图展示了在Goals Scored和Distance coverd两个特征的任意组合上的预测结果。 比如，当一个球队得了至少一分，并且跑通的总距离接近100km的时候，模型预测的结果最高。如果得了0分，跑动的距离就没什么用了。你能从决策树中看出这一结果吗？ 但是如果球队得分了的话，跑动的距离是会影响到预测的。确定你可以从2D的部分依赖图中看出这一个结果。你能从决策树中看出同样的结果吗? 练习kaggle小练习]]></content>
      <categories>
        <category>可解释性</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>data mining</tag>
        <tag>可解释性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Permutaion Importance —— 排列重要性]]></title>
    <url>%2F2019%2F08%2F17%2Fpermutaion-importance%2F</url>
    <content type="text"><![CDATA[转载请注明来源：http://iceflameworm.github.io/2019/08/17/permutation-importance/ 在推广数据分析、挖掘以及模型训练结果的时候，经常遇到客户或业务方需要我们对其进行解读。如果不能让客户或业务方很好地理解或者接受的话，“数据民工们”的工作成果就很难被有效地推行下去，哎，宝宝们都很苦啊。 最近忽然想起之前在逛kaggle的时候看到过有模型解释性相关的课程，于是就回头温习下，梳理下要点，一来加深自己的理解，也方便感兴趣的同学阅读。 这是第二节：Permutaion Importance 作用与特点训练得到一个模型之后，我们可能会问的一个最基本的问题是 哪些特征对预测结果的影响最大？ 这一概念叫做 特征重要性。 有很多度量特征重要性的方法。一些方法回答的问题与上述问题略有不同，而另外一些方法则具有一些documented shortcomings (暂译为：记录的缺点)。 与其它方法相比，排列重要性具有以下优点： 计算速度快 应用广泛、易于理解 与我们期望一个特征重要性度量所具有的性质一致 工作原理排列重要性使用模型的方式与你迄今为止所见到过的都不同，而且在一开始，很多人都会对其感到很困惑。所以首先举一个例子来具体介绍以下它。 假定有以下格式的数据集： 我们想用一个人10岁的数据去预测他20岁的身高是多少？ 数据中包含： 有用的特征（10岁时的身高） 较弱的特征（10岁时拥有的股票） 对预测基本没有作用的特征 排列重要性是要在模型拟合之后才能进行计算。 所以对于给定的身高、股票数量等取值之后，计算排列重要性并不会改变模型或者是它的预测结果。 相反，我们会问以下问题：如果随机打乱验证数据某一列的值，保持目标列以及其它列的数据不变，那么这种操作会在这些打乱的数据上对预测准确率产生怎样的影响？ 对某一列进行随机排序应当会降低预测的准确率，这是因为产生的数据不再对应于现实世界中的任何东西。如果随机打乱的那一列模型预测对其依赖程度很高，那么模型准确率的衰减程度就会更大。在这个例子中，打乱height at age 10将会让预测结果非常差。但是如果我们随机打乱的是socks owned，那么产生的预测结果就不会衰减得那么厉害。 有了上述认识之后，排列重要性就按照以下步骤进行计算： 得到一个训练好的模型 打乱某一列数据的值，然后在得到的数据集上进行预测。用预测值和真实的目标值计算损失函数因为随机排序升高了多少。模型性能的衰减量代表了打乱顺序的那一列的重要程度。 将打乱的那一列复原，在下一列数据上重复第2步操作，直到计算出了每一列的重要性。 代码示例下面的例子会用到这样一个模型，这个模型用球队的统计数据预测一个足球队会不会出现“全场最佳球员”。“全场最佳球员”奖是颁发给比赛里表现最好的球员的。我们现在关注的并不是建模的过程，所以下面的代码只是载入了数据，然后构建了一个很基础的模型。 1234567891011import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierdata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')y = (data['Man of the Match'] == "Yes") # Convert from string "Yes"/"No" to binaryfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]X = data[feature_names]train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) 下面演示如何用eli5库计算和展示排列重要性。 12345import eli5from eli5.sklearn import PermutationImportanceperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)eli5.show_weights(perm, feature_names = val_X.columns.tolist()) 输出结果： 排列重要性结果解读排在最上面的是最重要的特征，排在最下面是重要性最低的特征。 每一行的第一个数字表示模型性能衰减了多少（在这个例子中，使用准确率作为性能度量）。 跟数据科学里面的很多事情一样，在对某一打乱的特征提取重要性的时候，是存在随机性的，所以我们在计算排列重要性的时候，会通过多次打乱顺序的方式重复这一过程。在&plusmn;后面的数字表示标准差。 偶尔你会看到负值的排列重要性。在这些情况中，在打乱的数据上得到预测结果比真实数据的准确率更高。这在所选特征与目标基本无关（重要性应该为0）的情况下会出现，但是随机的因素导致预测结果在打乱的数据上表现得更准确。就像这个例子一样，因为没有容忍随机性的空间，这种情况在小的数据集上很常见。 在我们的例子中，最重要的特征是Goals scored，看上去似乎是说得通的。对于其他变量的排序是否令人意外，足球球迷可能会有比较直观的感觉。 练习kaggle小练习]]></content>
      <categories>
        <category>可解释性</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>data mining</tag>
        <tag>可解释性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Use cases for model insights —— 模型可解释性的应用场景]]></title>
    <url>%2F2019%2F08%2F15%2Fuse-cases-of-model-insights%2F</url>
    <content type="text"><![CDATA[转载请注明来源：http://iceflameworm.github.io/2019/08/15/use-cases-of-model-insights/ 在推广数据分析、挖掘以及模型训练结果的时候，经常遇到客户或业务方需要我们对其进行解读。如果不能让客户或业务方很好地理解或者接受的话，“数据民工们”的工作成果就很难被有效地推行下去，哎，宝宝们都很苦啊。 最近忽然想起之前在逛kaggle的时候看到过有模型解释性相关的课程，于是就回头温习下，梳理下要点，一来加深自己的理解，也方便感兴趣的同学阅读。 这是第一节：Use cases for model insights 做什么？很多人把各种机器学习模型看作是黑盒子，因为虽然这些模型给出的预测结果还不错，但是你却无法理解这些预测背后的逻辑。 要加深对模型结果的理解，可以从以下几个方面入手 训练出的模型认为哪些特征很重要？ 对于任意一个数据样本，每一个特征是如何影响其预测结果的。 从宏观意义上讲，每个特征是如何影响模型在整体上的预测结果的。 应用场景对模型结果的解释可应用到以下这些场景当中： 模型调试 特征工程 数据收集 决策制定 建立信任 模型调试在实际应用场景中，往往会存在很多不可靠、组织混乱和存在污染的数据。在对数据进行预处理的时候，很有可能在不经意间就会引入潜在的错误源。在实际的数据科学项目中，在某些地方出现错误是很正常的事情。 因为bug会频繁出现，且可能会引起灾难性的后果，所以模型调式数据科学领域最重要的技能之一。理解模型发现的模式会帮你确定什么时候它们跟你所掌握的知识不一致，而这通常才是追踪bug原因第一步而已。 特征工程一般情况下，特征工程是提升模型准确率最有效的方法。通常，特征工程需要不断地在原有数据或已创建的特征上进行变换，来产生新的特征。 在数据量、维度都比较小的情况下，有时你只靠自身对问题的直觉，就可以完成这一过程。但是当你需要处理成百上千的原始特征，或者并不太了解问题的背景的时候，你就需要更多的指导和建议了。 数据收集对于从网上下载的数据集，你是无法控制的。但是很多使用数据科学的企业和组织都有机会扩展所收集数据的类型。因为收集新类型的数据可能成本会很高，或者非常麻烦，所以只有在清楚这么做是划算的时候，企业和组织才会去做。基于模型的解释会帮你更好地理解现有特征地价值，进而推断出哪些新数据可能是最有帮助的。 决策制定某些情况下，模型会直接自动做出决策，但是有很多重要的决策是需要人来确定。对于最终需要人来做的决策，模型的可解释性比单纯的预测结果更重要。 建立信任许多人在确定一些基本的事实之前，不会信赖你用来做重要决策的模型。鉴于数据错误的频繁出现，这是一种明智的防范措施。在实际业务场景中，如果给出的模型解释符合对方自身对问题的理解，那么即使在基本不具备深入的数据科学知识的人之间，也将有助于建立互相信任的关系。]]></content>
      <categories>
        <category>可解释性</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>data mining</tag>
        <tag>可解释性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源框架gunthercox/ChatterBot浅析]]></title>
    <url>%2F2019%2F08%2F12%2Fchatterbot-github%2F</url>
    <content type="text"><![CDATA[转载请注明来源：http://iceflameworm.github.io/2019/08/12/chatterbot-github/ 框架简介Chatterbot是一个完全用python编写的基于文本检索/匹配的聊天机器人框架，它会从保存的对话语料中找出与输入句子最匹配的句子，并把匹配到的句子的下一句作为回答返回。本文主要对其工作流程，以及核心的训练器和逻辑适配器进行梳理，具体使用方法，请参考其文档。 框架地址：https://github.com/gunthercox/ChatterBot文档地址：https://chatterbot.readthedocs.io/en/stable/ 工作流程 原文档中有两幅描述工作流程的示意图，一幅在文档首页，一幅在文档-逻辑适配器，个人认为后者描述的更全面、更恰当些，所以就以后者为准进行介绍。 从输入句子到输出响应回答，前后需要经历三大步： 预处理 生成答案 答案选择 具体流程请参考chatterbot.py。 预处理与通常所讲的NLP预处理的目的基本一致，主要是文本进行一些标准还操作，比如去除连续的空格、删除特殊字符等等。Chatterbot框架自身实现了clean_whitespace, unescape_html和convert_to_ascii三种预处理功能。具体实现参见preprocessors.py 生成答案一个Chatterbot实例可以绑定多个逻辑适配器，用于根据输入产生输出。 Chatterbot中没有独立的用于选择对话逻辑的意图识别模块，它将意图识别的功能放到了各个逻辑适配器中。接收到输入之后，Chatterbot会将其传递给各个逻辑适配器，由它们自己判断是否适合对输入的文本进行回答。如果逻辑适配器认为不能对输入进行回答，则会跳过，否则就输出回答，这样的话，有可能所有逻辑适配器都不输出回答，也有可能有多个逻辑适配器都给出了回答。具体请参考：ChatBot::generate_response方法。 从ChatBot::__init__和Chatbot::generate_response中的两端代码 12345678910# __init__self.storage = utils.initialize_class(storage_adapter, **kwargs)...# generate_responsefor adapter in logic_adapters: utils.validate_adapter_class(adapter, LogicAdapter) logic_adapter = utils.initialize_class(adapter, self, **kwargs) self.logic_adapters.append(logic_adapter) 可知，所有的逻辑适配器都共享一份保存的对话语料。倘若逻辑适配器内部不对数据进行选择的话，所有的逻辑适配器都将从所有的对话语料数据中查找最匹配的回答。这样的结果就是，每个逻辑适配器在相同的数据上用不同的匹配方法或指标产生回答，衡量每个回答confidence的标准并不一致，这会影响后续根据confidence进行答案选择。 答案选择工作流程示意图显示，Chatterbot会从所有的逻辑适配器返回的回答中，选择confidence最大的，但是ChatBot::generate_response实现的是另外一种逻辑。在Chatbot::generate_response中，每个逻辑适配器输出的confidence并没有用到，它会统计每一个返回的回答出现的次数，如果有出现次数大于1次的，则会返回出现次数最多的回答，但是如果所有逻辑适配器返回的回答都只出现了一次，则会第一个逻辑适配器的答案，个人认为这种逻辑存在缺陷。以下是相关代码： 123456789101112131415161718192021222324# If multiple adapters agree on the same statement,# then that statement is more likely to be the correct responseif len(results) &gt;= 3: result_options = &#123;&#125; for result_option in results: result_string = result_option.text + ':' + (result_option.in_response_to or '') if result_string in result_options: result_options[result_string].count += 1 if result_options[result_string].statement.confidence &lt; result_option.confidence: result_options[result_string].statement = result_option else: result_options[result_string] = ResultOption( result_option ) most_common = list(result_options.values())[0] for result_option in result_options.values(): if result_option.count &gt; most_common.count: most_common = result_option if most_common.count &gt; 1: result = most_common.statement 训练器刚开始的时候，以为这里的训练跟普通的训练是一样的，也就是通过数据+训练过程确定模型的参数。实际上，这里的训练过程不能算作真正的训练，有点跟KNN算法的训练过程差不多（KNN也没有真正意义上的训练过程），所谓的训练过程其实就是准备检索数据的过程。结合文档示例和 ListTrainer 可以看出Chatterbot框架中的训练过程实际上是怎样的。 文档中的示例 1234567891011121314from chatbot import chatbotfrom chatterbot.trainers import ListTrainertrainer = ListTrainer(chatbot)trainer.train([ "Hi there!", "Hello",])trainer.train([ "Greetings!", "Hello",]) ListTrainer 1234567891011121314151617181920212223242526272829303132333435363738394041class ListTrainer(Trainer): """ Allows a chat bot to be trained using a list of strings where the list represents a conversation. """ def train(self, conversation): """ Train the chat bot based on the provided list of statements that represents a single conversation. """ previous_statement_text = None previous_statement_search_text = '' statements_to_create = [] for conversation_count, text in enumerate(conversation): if self.show_training_progress: utils.print_progress_bar( 'List Trainer', conversation_count + 1, len(conversation) ) statement_search_text = self.chatbot.storage.tagger.get_text_index_string(text) statement = self.get_preprocessed_statement( Statement( text=text, search_text=statement_search_text, in_response_to=previous_statement_text, search_in_response_to=previous_statement_search_text, conversation='training' ) ) previous_statement_text = statement.text previous_statement_search_text = statement_search_text statements_to_create.append(statement) self.chatbot.storage.create_many(statements_to_create) 对话语料的保存并不是以文本对为单位的，而是把每一句话作为最基本的存储单元，语句之间的关系通过*in_response_to字段表示。如下述代码所示，Statement本身的text是用来回答in_response_to对应的previous_statement_text这句话的。 1234567Statement( text=text, search_text=statement_search_text, in_response_to=previous_statement_text, search_in_response_to=previous_statement_search_text, conversation='training') 逻辑适配器逻辑适配器主要用于根据输入文本产生相应的回答。Chatterbot本身实现了一个BestMatch的逻辑适配器，它会从保存的对话语料中找出与输入文本最匹配的回答，其基本检索流程是： 根据某一相似度度量指标，从保存的对话语料中找到与输入文本最相似的文本 mtext。 遍历整个对话语料，找出所有可以用来回答m_text的文本。 从上述步骤可以看出， BestMatch找到回答需要遍历两次保存的对话语料，在实现方式上可能不是最优的。 具体流程参考 BestMatch::process。 在文本相似度度量上，Chatterbot本身已经实现了多种不同的方法： LevenshteinDistance-编辑距离 JaccardSimilarity 使用Spacy计算的相似度 具体请参考 comparisons.py。]]></content>
      <categories>
        <category>chatbot</category>
      </categories>
      <tags>
        <tag>开源框架</tag>
        <tag>NLP</tag>
        <tag>chatbot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[闻言、明义、知心——你的智能伙伴，聊天机器人]]></title>
    <url>%2F2019%2F08%2F08%2Fchatbot-wechat%2F</url>
    <content type="text"><![CDATA[第一篇真正的博文，就转发一下自己为公司写的软文吧，闻言、明义、知心——你的智能伙伴，聊天机器人 “Hello！I am Baymax，Your personal healthcare companion.” （“你好，我是大白，你的个人健康伙伴”）——《超能陆战队》。 《超能陆战队》是一部于2015年上映的动漫科幻电影，成功塑造了“大白”这样一个憨态可掬、善解人意的智能机器人的形象。与冷冰冰的机器相比，大白不仅能跟主人公小宏进行萌萌哒的语言交流，而且还能感受、理解小宏的意图、情绪和情感，帮助小宏克服种种困难，完成各种挑战。假如现在也给我们这样一个“大白”的话，我想你我应该都不会拒绝的。 虽然到目前为止， 现实中还不存在像“大白”这样一个暖心的智能伙伴，但是人工智能技术地发展，催生了各种聊天机器人的出现（小冰、小娜、小蜜、小爱同学、Siri、balabala…），正让这位暖心的朋友一步一步地向我们走来。现在这些还活在我们手机、电脑或音箱里的聊天机器人已经可以帮我们做一些类似于机票预定、问题解答等相关的事情了，甚至它们还会不时地“撩”你一下哟。 环肥燕瘦 自2016年谷歌I/O开发者大会发布Google Home智能音箱以来，国内外掀起了一股聊天机器人产品化的热潮，国内外的百度、阿里、小米、亚马逊、微软等知名厂商纷纷发布了各自的聊天机器人产品和开放平台。微软的当家人萨提亚·纳德拉更是提出了对话即平台（Conversations as a Platform， CaaS）的发展战略。 现在已经迈入2019年下半年的门槛了，到目前为止，市面上涌现了各式各样的聊天机器人产品。这些聊天机器人可能是一个圆圆或者方方的智能音箱，安静的待在角落里时刻等待着你的召唤。它们还可能藏身于一部手机、一块手表，时刻陪在你的身边，帮你订外卖、问天气、查路线。有的时候它们会化身为“人”，热情地欢迎你的光临。或许你不是一个张扬的人，不喜欢把一切都说出来，它们也会耐心地跟你用文字传递心意。 多才多艺 各种各样的聊天机器人产品正通过语言和文字这两种最接近人类交互的方式，慢慢地融入到我们生活、学习、工作当中，让我们身边的一切都开始变得智能起来。目前，聊天机器人已经在智能家居、智能生活、智能客服等领域取得了初步的成效。未来，在更多的领域，它们都会像一位专属于你的私人秘书一样，只要你跟他简单地“说”一句，它们就会理解你的想法，帮你完成想做的事情。 智能家居 目前很多的智能音箱产品或者语音助手都提供了家居场景的解决方案，用户可以通过人机对话的方式对家庭中各类电子设备的控制，如家居机器人、灯光、冰箱、插座、空调、电视、窗帘等等，彻底解放双手。想象一下，未来在自己的家中，我们或许真的能过上“衣来伸手饭来张口”的日子。 智能生活 现在是移动互联网的时代，智能手机与各种智能穿戴设备都已经非常流行。在系统级别上，用户在唤醒手机助手或智能设备后，可以通过对话的方式进行交流、操作和控制，获取资讯以及各项生活服务 等等。在app级别上，用户亦可以通过对话的方式，免手持对app进行功能操作或内容获取等。现在我们已经可以通过语音助手查天气、订外卖、订机票、打电话、发短信等等一系列事情了，未来，随着技术的不断进步，我们可能会拥有一位更全能的智能“生活管家”。 智能客服 现在各种网站以及app开始越来越多的把聊天机器人集成到客服系统中，相对于人工座席客服，聊天机器人可以提供7*24不间断的服务。目前机器人客服已经可以代替人工客服解答一些高频的问题，使得人工客服能够聚焦于更高价值的业务，有效地降低了人力成本，提升了人效。 其它 除了上述应用场景外，聊天机器人还被越来越多地应用于智能出行、智能服务等场景中，随着人工智能的不断发展，我们可能会在越来越多的场景中看到聊天机器人的身影。 三大流派聊天机器人的形态多种多样，交互的方式也不尽相同，但是对所包含的功能进行划分的话，总体上可以分为任务型、问答型和闲聊型三种。有些机器人产品只包含某一种类型的功能，而另外一些则会集成其中两种或所有类型的功能。 任务型任务型机器人指特定条件下提供信息或服务的机器人。通常情况下是为了满足带有明确目的的用户，例如查流量，查话费，订餐，订票，咨询等任务型场景。由于用户的需求较为复杂，通常情况下需分多轮互动，用户也可能在对话过程中不断修改与完善自己的需求，任务型机器人需要通过询问、澄清和确认来帮助用户明确目的。其基本流程如下： 口语理解（SLU）：把用户输入的自然语言转变为结构化信息——act-slot-value三元组。例如餐厅订座应用中用户说“订云海肴中关村店”，我们通过NLU把它转化为结构化信息：“inform(order_op=预订, restaurant_name=云海肴, subbranch=中关村店)”，其中的“inform”是动作名称，而括号中的是识别出的槽位及其取值。 对话管理（DM）：综合用户当前query和历史对话中已获得的信息后，给出机器答复的结构化表示。对话管理包含两个模块：对话状态追踪（DST）和策略优化（DPO）。 DST维护对话状态，它依据最新的系统和用户行为，把旧对话状态更新为新对话状态。其中对话状态应该包含持续对话所需要的各种信息。 DPO根据DST维护的对话状态，确定当前状态下机器人应如何进行答复，也即采取何种策略答复是最优的。 自然语言产生（NLG）：把DM输出的结构化对话策略还原成对人友好的自然语言。简单的NLG方法可以是事先设定好的回复模板，复杂的可以使用深度学习生成模型。 问答型问答型机器人的主要任务是从特定知识库中找出与用户提出的问题最匹配的答案。智能客服通常都会包含这一类型的机器人，用于回答一些高频的问题，减轻人工客服的压力。其基本流程如下： 文本检索和匹配是问答型机器人的核心组成部分，除此之外，可能还会重排序这一功能模块。 文本检索：首先通过检索的方式从知识库的大量语料中筛选出可能包含答案的候选集，供后续匹配模块打分，计算相似度。文本检索主要包含词语级别和句子级别两种方式，其中词语级别是通过关键词匹配进行的，句子级别是基于语义度量进行的。 文本匹配：通过字面或者语义等不同的方式计算用户所提问题与候选集的相似度。这些相似度会直接用于返回答案或者供后续重排序模块使用。 重排序：类似于ensemble learning中的stacking，利用各种不同类型的相似度分数作为输入，重新对候选集进行排序，排序的结果将最终用于生成答案。 闲聊型真实应用中，用户与系统交互的过程中不免会涉及到闲聊成分。闲聊功能可以让对话机器人更有情感和温度。闲聊机器人可以通过事先准备的闲聊库实现，这样的话，类似于问答型的机器人。另外一种实现方式是使用机器翻译中的深度学习seq2seq框架来产生答复，相比于前一种实现方式，这种方式通常能产生更多样化的回答，智能化程度更高。 结语聊天机器人承载了全新的交互形式，可能带来了全新的产品服务体验，这种进步过去从来没有过。我们每个人几乎都在渴望着机器人时代的到来，也许很遥远，也许已经在路上，谁知道呢？Hello，大白，你快到了吗？ 参考资料 小米AI音箱：https://www.mi.com/aispeaker/ 天猫精灵：https://bot.tmall.com/ unit开放平台：https://ai.baidu.com/unit/home 关于对话机器人，你需要了解这些技术：https://blog.csdn.net/qq_40027052/article/details/78723576 揭秘任务型对话机器人（上篇）：https://www.cnblogs.com/qcloud1001/p/9181900.html AnyQ：https://github.com/baidu/AnyQ]]></content>
      <categories>
        <category>chatbot</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>chatbot</tag>
      </tags>
  </entry>
</search>
